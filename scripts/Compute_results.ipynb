{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/distributed/utils.py:134: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable\n",
      "  % (host, default, e), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import prepro, metrics, run, setup\n",
    "import src.models.factory as model_factory\n",
    "import config\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from src.utils import models\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src.utils.data_processing import *\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#args = setup.create_setup()\n",
    "#args.mode = 'displacement'\n",
    "#args.full_encoder = True\n",
    "#args.encoder_config = 'full_encoder_config'\n",
    "#args.decoder_config = 'lstm_config'\n",
    "window_size = 8\n",
    "predict_at = 16\n",
    "#args.target_intensity_cat = False\n",
    "#args.sub_window_size = 8\n",
    "#args.sub_area = 1\n",
    "#args.output_dir = '../results/results8_16_20_44_28' #''../results/results8_16_18_57_7' best so far\n",
    "#try: ../results/results8_16_20_44_28, 0.086 from training\n",
    "#Reached 70.4 and the best results with combination. 7.47 GRU outputs 64. All rest normal.\n",
    "#args.output_dir = './results/results7_20_17_12_19' #Be careful to change 2304 for intermediary layer\n",
    "#args.output_dir = './results/results7_20_15_4_36' : best for acc 70.5 and nearly best for intensiy 7.50\n",
    "#args. = ./results/results8_12_19_5_42 for best perf t+48h\n",
    "\n",
    "#if args.sub_area > 0:\n",
    "    #x_viz_train = x_viz_train[:, :, :, args.sub_area:-args.sub_area, args.sub_area:-args.sub_area]\n",
    "    #x_viz_test = x_viz_test[:, :, :, args.sub_area:-args.sub_area, args.sub_area:-args.sub_area]\n",
    "\n",
    "tgt_intensity_cat_train = torch.LongTensor(np.load('../data/y_train_intensity_cat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                      allow_pickle=True))\n",
    "tgt_intensity_cat_test = torch.LongTensor(np.load('../data/y_test_intensity_cat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                     allow_pickle=True))\n",
    "\n",
    "tgt_intensity_train = torch.Tensor(np.load('../data/y_train_intensity_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                  allow_pickle=True))\n",
    "tgt_intensity_test = torch.Tensor(np.load('../data/y_test_intensity_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                 allow_pickle=True))\n",
    "\n",
    "tgt_intensity_cat_baseline_train = torch.LongTensor(np.load('../data/y_train_intensity_cat_baseline_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',  allow_pickle = True))\n",
    "tgt_intensity_cat_baseline_test = torch.LongTensor(np.load('../data/y_test_intensity_cat_baseline_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "\n",
    "tgt_displacement_train = torch.Tensor(np.load('../data/y_train_displacement_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                     allow_pickle=True))\n",
    "tgt_displacement_test = torch.Tensor(np.load('../data/y_test_displacement_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                    allow_pickle=True))\n",
    "\n",
    "tgt_displacement_train_unst = torch.Tensor(np.load('../data/y_train_displacement_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                     allow_pickle=True))\n",
    "tgt_displacement_test_unst = torch.Tensor(np.load('../data/y_test_displacement_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                    allow_pickle=True))\n",
    "\n",
    "\n",
    "mean_intensity = tgt_intensity_train.mean()\n",
    "std_intensity = tgt_intensity_train.std()\n",
    "tgt_intensity_train = (tgt_intensity_train - mean_intensity)/std_intensity\n",
    "tgt_intensity_test = (tgt_intensity_test - mean_intensity)/std_intensity\n",
    "\n",
    "\n",
    "###INTENSITY\n",
    "mean_dx = tgt_displacement_train[:,0].mean()\n",
    "std_dx = tgt_displacement_train[:,0].std()\n",
    "tgt_displacement_train[:,0] = (tgt_displacement_train[:,0] - mean_dx)/std_dx\n",
    "tgt_displacement_test[:,0] = (tgt_displacement_test[:,0] - mean_dx)/std_dx\n",
    "std_dx = float(std_dx)\n",
    "mean_dx = float(mean_dx)\n",
    "\n",
    "mean_dy = tgt_displacement_train[:,1].mean()\n",
    "std_dy = tgt_displacement_train[:,1].std()\n",
    "tgt_displacement_train[:,1] = (tgt_displacement_train[:,1] - mean_dy)/std_dy\n",
    "tgt_displacement_test[:,1] = (tgt_displacement_test[:,1] - mean_dy)/std_dy\n",
    "std_dy = float(std_dy)\n",
    "mean_dy = float(mean_dy)\n",
    "\n",
    "def standardize(tgt_displacement_train, tgt_displacement_test):\n",
    "    mean_dx = tgt_displacement_train[:, 0].mean()\n",
    "    std_dx = tgt_displacement_train[:, 0].std()\n",
    "    tgt_displacement_train[:, 0] = (tgt_displacement_train[:, 0] - mean_dx) / std_dx\n",
    "    tgt_displacement_test[:, 0] = (tgt_displacement_test[:, 0] - mean_dx) / std_dx\n",
    "    std_dx = float(std_dx)\n",
    "    mean_dx = float(mean_dx)\n",
    "    mean_dy = tgt_displacement_train[:, 1].mean()\n",
    "    std_dy = tgt_displacement_train[:, 1].std()\n",
    "    tgt_displacement_train[:, 1] = (tgt_displacement_train[:, 1] - mean_dy) / std_dy\n",
    "    tgt_displacement_test[:, 1] = (tgt_displacement_test[:, 1] - mean_dy) / std_dy\n",
    "    std_dy = float(std_dy)\n",
    "    mean_dy = float(mean_dy)\n",
    "    return tgt_displacement_train, tgt_displacement_test, std_dx, mean_dx, std_dy, mean_dy\n",
    "\n",
    "def unstandardize(tgt_displacement_train, tgt_displacement_test, std_dx, mean_dx, std_dy, mean_dy):\n",
    "    tgt_displacement_train[:, 0] = tgt_displacement_train[:, 0] *  std_dx + mean_dx\n",
    "    tgt_displacement_test[:, 0] = tgt_displacement_test[:, 0] *  std_dx + mean_dx\n",
    "    tgt_displacement_train[:, 1] = tgt_displacement_train[:, 1] * std_dy + mean_dy\n",
    "    tgt_displacement_test[:, 1] = tgt_displacement_test[:, 1] * std_dy + mean_dy\n",
    "    return tgt_displacement_train, tgt_displacement_test\n",
    "\n",
    "\n",
    "##########\n",
    "##########\n",
    "########## PREPARING DATA FOR XGB\n",
    "\n",
    "X_train = np.load('../data/X_train_stat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "            allow_pickle=True)\n",
    "X_test = np.load('../data/X_test_stat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "            allow_pickle=True)\n",
    "\n",
    "\n",
    "names = ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND',\n",
    "         'STORM_SPEED', 'cat_cos_day', 'cat_sign_day', 'COS_STORM_DIR', 'SIN_STORM_DIR',\n",
    "         'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'cat_storm_category', 'cat_basin_AN',\n",
    "         'cat_basin_EP', 'cat_basin_NI', 'cat_basin_SA',\n",
    "         'cat_basin_SI', 'cat_basin_SP', 'cat_basin_WP', 'cat_nature_DS', 'cat_nature_ET',\n",
    "         'cat_nature_MX', 'cat_nature_NR', 'cat_nature_SS', 'cat_nature_TS',\n",
    "         'STORM_DISPLACEMENT_X', 'STORM_DISPLACEMENT_Y']\n",
    "\n",
    "names_all = names * window_size\n",
    "\n",
    "for i in range(len(names_all)):\n",
    "    names_all[i] += '_' + str(i // 30)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_train.columns = names_all\n",
    "X_test.columns = names_all\n",
    "\n",
    "cols = [c for c in X_train.columns if c.lower()[-2:] == '_0' or c.lower()[:3] != 'cat']\n",
    "\n",
    "X_train = X_train[cols]\n",
    "X_test = X_test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_embed = np.load('../data/embeddings/X_train_embeds_1980_34_20_120_results8_16_20_44_28.npy', allow_pickle = True)\n",
    "#X_test_embed = np.load('../data/embeddings/X_test_embeds_1980_34_20_120_results8_16_20_44_28.npy', allow_pickle = True)\n",
    "#X_train_embed = np.load('../data/embeddings/X_train_embed_1980_34_20_120_intensity.npy', allow_pickle = True)\n",
    "#X_test_embed = np.load('../data/embeddings/X_test_embed_1980_34_20_120_intensity.npy', allow_pickle = True)\n",
    "\n",
    "#48\n",
    "#X_train_embed = np.load('../data/embeddings/X_train_embed_1980_34_20_120_intensity_48.npy', allow_pickle = True)\n",
    "#X_test_embed = np.load('../data/embeddings/X_test_embed_1980_34_20_120_intensity_48.npy', allow_pickle = True)\n",
    "\n",
    "X_train_embed = np.load('../data/embeddings/X_train_embed_1980_34_20_120_track_48.npy', allow_pickle = True)\n",
    "X_test_embed = np.load('../data/embeddings/X_test_embed_1980_34_20_120_track_48.npy', allow_pickle = True)\n",
    "\n",
    "X_train_total = np.concatenate((X_train, X_train_embed), axis = 1)\n",
    "X_test_total = np.concatenate((X_test, X_test_embed), axis = 1)\n",
    "\n",
    "std_ = float(std_intensity)\n",
    "mean_ = float(mean_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107199, 512)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE intensity:  12.559202\n",
      "MAE intensity:  11.940126\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBRegressor(max_depth=6, n_estimators=140, learning_rate = 0.07, subsample = 0.7, min_child_weight = 5)\n",
    "xgb2.fit(X_train, tgt_intensity_train)\n",
    "print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_intensity_test)*std_+mean_, np.array(xgb2.predict(X_test))*std_+mean_))\n",
    "\n",
    "xgb = XGBRegressor(max_depth=8, n_estimators = 150, learning_rate = 0.07, subsample = 0.9)\n",
    "xgb.fit(X_train_total, tgt_intensity_train)\n",
    "print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_intensity_test)*std_+mean_, np.array(xgb.predict(X_test_total))*std_+mean_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE intensity:  7.6776776\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(max_depth=8, n_estimators = 150, learning_rate = 0.07, subsample = 0.7)\n",
    "xgb.fit(X_train_embed, tgt_intensity_train)\n",
    "print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_intensity_test)*std_+mean_, np.array(xgb.predict(X_test_embed))*std_+mean_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE intensity:  7.747515\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(max_depth=8, n_estimators = 120, learning_rate = 0.07, subsample = 0.7)\n",
    "xgb.fit(X_train_total, tgt_intensity_train)\n",
    "print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_intensity_test)*std_+mean_, np.array(xgb.predict(X_test_total))*std_+mean_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE intensity:  5.0198307\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_intensity_train)*std_+mean_, np.array(xgb.predict(X_train_total))*std_+mean_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE intensity:  11.97465\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(max_depth=8, n_estimators = 140, learning_rate = 0.07, subsample = 0.8)\n",
    "xgb.fit(X_train_total, tgt_intensity_train)\n",
    "print(\"MAE intensity: \", mean_absolute_error(np.array(tgt_intensity_test)*std_+mean_, np.array(xgb.predict(X_test_total))*std_+mean_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_24_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "#X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_all_48_clean_w8_at_16.npy', allow_pickle=True))\n",
    "\n",
    "X_test_dates = pd.DataFrame(np.load('../data/X_test_stat_with_dates_columns_1980_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True)[:,:4])\n",
    "\n",
    "X_test_dates.columns = ['YEAR', 'MONTH', 'DAY', 'HOUR']\n",
    "\n",
    "names_baselines = ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED', 'cos_day', 'sin_day', 'COS_STORM_DIR', 'SIN_STORM_DIR', 'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'wind_category', 'OFCL_24_lat', 'OFCL_24_lon', 'OFCL_24_vmax', 'OFCL_24_mslp', 'P91E_24_lat', 'P91E_24_lon', 'P91E_24_vmax', 'P91E_24_mslp', 'GFDL_24_lat', 'GFDL_24_lon', 'GFDL_24_vmax', 'GFDL_24_mslp', 'NGPS_24_lat', 'NGPS_24_lon', 'NGPS_24_vmax', 'NGPS_24_mslp', 'LBAR_24_lat', 'LBAR_24_lon', 'LBAR_24_vmax', 'LBAR_24_mslp', 'GUNS_24_lat', 'GUNS_24_lon', 'GUNS_24_vmax', 'GUNS_24_mslp', 'UKXI_24_lat', 'UKXI_24_lon', 'UKXI_24_vmax', 'UKXI_24_mslp', 'EMXI_24_lat', 'EMXI_24_lon', 'EMXI_24_vmax', 'EMXI_24_mslp', 'A98E_24_lat', 'A98E_24_lon', 'A98E_24_vmax', 'A98E_24_mslp', 'SHIP_24_lat', 'SHIP_24_lon', 'SHIP_24_vmax', 'SHIP_24_mslp', 'DSHP_24_lat', 'DSHP_24_lon', 'DSHP_24_vmax', 'DSHP_24_mslp', 'FSSE_24_lat', 'FSSE_24_lon', 'FSSE_24_vmax', 'FSSE_24_mslp', 'CLP5_24_lat', 'CLP5_24_lon', 'CLP5_24_vmax', 'CLP5_24_mslp', 'AEMN_24_lat', 'AEMN_24_lon', 'AEMN_24_vmax', 'AEMN_24_mslp', 'CMC_24_lat', 'CMC_24_lon', 'CMC_24_vmax', 'CMC_24_mslp', 'GFSO_24_lat', 'GFSO_24_lon', 'GFSO_24_vmax', 'GFSO_24_mslp', 'HWRF_24_lat', 'HWRF_24_lon', 'HWRF_24_vmax', 'HWRF_24_mslp', 'cat_basin_AN', 'cat_basin_EP', 'basin_NI', 'basin_SA', 'basin_SI', 'cat_basin_SP', 'cat_basin_WP', 'DISPLACEMENT_LAT', 'DISPLACEMENT_LON']\n",
    "\n",
    "names_all_baselines = names_baselines * 8#args.window_size\n",
    "\n",
    "for i in range(len(names_all_baselines)):\n",
    "    names_all_baselines[i] += '_' + str(i // 92)\n",
    "\n",
    "X_test_baseline.columns = names_all_baselines\n",
    "\n",
    "#X_test_baseline = pd.concat([X_test_baseline, X_test_dates], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25922])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_intensity_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/X_test_stat_1980_34_20_120_forecast_48_2012_v2_w8_at_16.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-87a6133b77c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_24_2012_v2_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_test_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/X_test_stat_1980_34_20_120_forecast_48_2012_v2_w'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_at_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_at\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnames_baselines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'YEAR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MONTH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HOUR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WMO_WIND'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WMO_PRES'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DIST2LAND'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'STORM_SPEED'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cos_day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sin_day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'COS_STORM_DIR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SIN_STORM_DIR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wind_category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFDL_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFDL_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFDL_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFDL_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFDL_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFDL_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFDL_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFDL_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CMC_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CMC_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CMC_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CMC_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CMC_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CMC_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CMC_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CMC_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FSSE_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FSSE_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FSSE_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FSSE_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FSSE_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FSSE_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FSSE_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FSSE_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OFCL_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OFCL_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OFCL_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OFCL_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OFCL_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OFCL_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OFCL_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OFCL_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NGPS_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NGPS_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NGPS_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NGPS_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NGPS_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NGPS_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NGPS_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NGPS_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DSHP_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DSHP_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DSHP_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DSHP_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DSHP_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DSHP_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DSHP_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DSHP_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SHIP_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SHIP_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SHIP_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SHIP_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SHIP_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SHIP_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SHIP_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SHIP_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLP5_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLP5_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLP5_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLP5_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLP5_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLP5_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLP5_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLP5_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HWRF_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HWRF_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HWRF_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HWRF_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HWRF_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HWRF_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HWRF_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HWRF_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UKXI_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UKXI_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UKXI_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UKXI_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UKXI_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UKXI_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UKXI_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UKXI_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LBAR_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LBAR_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LBAR_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LBAR_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LBAR_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LBAR_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LBAR_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LBAR_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AEMN_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AEMN_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AEMN_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AEMN_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AEMN_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AEMN_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AEMN_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AEMN_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_CLP5_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_CLP5_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_SHIP_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_SHIP_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_DSHP_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_DSHP_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_LBAR_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_LBAR_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_CMC_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_CMC_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_NGPS_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_NGPS_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_GFDL_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_GFDL_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_HWRF_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_HWRF_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_UKXI_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_UKXI_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_FSSE_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_FSSE_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_AEMN_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_AEMN_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_OFCL_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_OFCL_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMXI_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMXI_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMXI_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMXI_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMXI_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMXI_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMXI_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMXI_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_EMXI_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_EMXI_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFSO_24_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFSO_24_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFSO_24_vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFSO_24_mslp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFSO_24_COS_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFSO_24_SIN_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFSO_24_COS_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GFSO_24_SIN_LON'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT_GFSO_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON_GFSO_24'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat_basin_AN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat_basin_EP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basin_NI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basin_SI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basin_SP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'basin_WP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISPLACEMENT_LON'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/X_test_stat_1980_34_20_120_forecast_48_2012_v2_w8_at_16.npy'"
     ]
    }
   ],
   "source": [
    "#X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_24_2012_v2_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_48_2012_v2_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "\n",
    "\n",
    "names_baselines = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED', 'cos_day', 'sin_day', 'COS_STORM_DIR', 'SIN_STORM_DIR', 'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'wind_category', 'GFDL_24_lat', 'GFDL_24_lon', 'GFDL_24_vmax', 'GFDL_24_mslp', 'GFDL_24_COS_LAT', 'GFDL_24_SIN_LAT', 'GFDL_24_COS_LON', 'GFDL_24_SIN_LON', 'CMC_24_lat', 'CMC_24_lon', 'CMC_24_vmax', 'CMC_24_mslp', 'CMC_24_COS_LAT', 'CMC_24_SIN_LAT', 'CMC_24_COS_LON', 'CMC_24_SIN_LON', 'FSSE_24_lat', 'FSSE_24_lon', 'FSSE_24_vmax', 'FSSE_24_mslp', 'FSSE_24_COS_LAT', 'FSSE_24_SIN_LAT', 'FSSE_24_COS_LON', 'FSSE_24_SIN_LON', 'OFCL_24_lat', 'OFCL_24_lon', 'OFCL_24_vmax', 'OFCL_24_mslp', 'OFCL_24_COS_LAT', 'OFCL_24_SIN_LAT', 'OFCL_24_COS_LON', 'OFCL_24_SIN_LON', 'NGPS_24_lat', 'NGPS_24_lon', 'NGPS_24_vmax', 'NGPS_24_mslp', 'NGPS_24_COS_LAT', 'NGPS_24_SIN_LAT', 'NGPS_24_COS_LON', 'NGPS_24_SIN_LON', 'DSHP_24_lat', 'DSHP_24_lon', 'DSHP_24_vmax', 'DSHP_24_mslp', 'DSHP_24_COS_LAT', 'DSHP_24_SIN_LAT', 'DSHP_24_COS_LON', 'DSHP_24_SIN_LON', 'SHIP_24_lat', 'SHIP_24_lon', 'SHIP_24_vmax', 'SHIP_24_mslp', 'SHIP_24_COS_LAT', 'SHIP_24_SIN_LAT', 'SHIP_24_COS_LON', 'SHIP_24_SIN_LON', 'CLP5_24_lat', 'CLP5_24_lon', 'CLP5_24_vmax', 'CLP5_24_mslp', 'CLP5_24_COS_LAT', 'CLP5_24_SIN_LAT', 'CLP5_24_COS_LON', 'CLP5_24_SIN_LON', 'HWRF_24_lat', 'HWRF_24_lon', 'HWRF_24_vmax', 'HWRF_24_mslp', 'HWRF_24_COS_LAT', 'HWRF_24_SIN_LAT', 'HWRF_24_COS_LON', 'HWRF_24_SIN_LON', 'UKXI_24_lat', 'UKXI_24_lon', 'UKXI_24_vmax', 'UKXI_24_mslp', 'UKXI_24_COS_LAT', 'UKXI_24_SIN_LAT', 'UKXI_24_COS_LON', 'UKXI_24_SIN_LON', 'LBAR_24_lat', 'LBAR_24_lon', 'LBAR_24_vmax', 'LBAR_24_mslp', 'LBAR_24_COS_LAT', 'LBAR_24_SIN_LAT', 'LBAR_24_COS_LON', 'LBAR_24_SIN_LON', 'AEMN_24_lat', 'AEMN_24_lon', 'AEMN_24_vmax', 'AEMN_24_mslp', 'AEMN_24_COS_LAT', 'AEMN_24_SIN_LAT', 'AEMN_24_COS_LON', 'AEMN_24_SIN_LON', 'DISPLACEMENT_LAT_CLP5_24', 'DISPLACEMENT_LON_CLP5_24', 'DISPLACEMENT_LAT_SHIP_24', 'DISPLACEMENT_LON_SHIP_24', 'DISPLACEMENT_LAT_DSHP_24', 'DISPLACEMENT_LON_DSHP_24', 'DISPLACEMENT_LAT_LBAR_24', 'DISPLACEMENT_LON_LBAR_24', 'DISPLACEMENT_LAT_CMC_24', 'DISPLACEMENT_LON_CMC_24', 'DISPLACEMENT_LAT_NGPS_24', 'DISPLACEMENT_LON_NGPS_24', 'DISPLACEMENT_LAT_GFDL_24', 'DISPLACEMENT_LON_GFDL_24', 'DISPLACEMENT_LAT_HWRF_24', 'DISPLACEMENT_LON_HWRF_24', 'DISPLACEMENT_LAT_UKXI_24', 'DISPLACEMENT_LON_UKXI_24', 'DISPLACEMENT_LAT_FSSE_24', 'DISPLACEMENT_LON_FSSE_24', 'DISPLACEMENT_LAT_AEMN_24', 'DISPLACEMENT_LON_AEMN_24', 'DISPLACEMENT_LAT_OFCL_24', 'DISPLACEMENT_LON_OFCL_24', 'EMXI_24_lat', 'EMXI_24_lon', 'EMXI_24_vmax', 'EMXI_24_mslp', 'EMXI_24_COS_LAT', 'EMXI_24_SIN_LAT', 'EMXI_24_COS_LON', 'EMXI_24_SIN_LON', 'DISPLACEMENT_LAT_EMXI_24', 'DISPLACEMENT_LON_EMXI_24', 'GFSO_24_lat', 'GFSO_24_lon', 'GFSO_24_vmax', 'GFSO_24_mslp', 'GFSO_24_COS_LAT', 'GFSO_24_SIN_LAT', 'GFSO_24_COS_LON', 'GFSO_24_SIN_LON', 'DISPLACEMENT_LAT_GFSO_24', 'DISPLACEMENT_LON_GFSO_24', 'cat_basin_AN', 'cat_basin_EP', 'basin_NI', 'basin_SI', 'basin_SP', 'basin_WP', 'DISPLACEMENT_LAT', 'DISPLACEMENT_LON']\n",
    "\n",
    "names_all_baselines = names_baselines * 8#args.window_size\n",
    "\n",
    "for i in range(len(names_all_baselines)):\n",
    "    names_all_baselines[i] += '_' + str(i // 167)\n",
    "\n",
    "X_test_baseline.columns = names_all_baselines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_test_baseline.shape[0]\n",
    "X_test_total = X_test_total[-n:]\n",
    "tgt_intensity_test = tgt_intensity_test[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR_0</th>\n",
       "      <th>MONTH_0</th>\n",
       "      <th>DAY_0</th>\n",
       "      <th>HOUR_0</th>\n",
       "      <th>LAT_0</th>\n",
       "      <th>LON_0</th>\n",
       "      <th>WMO_WIND_0</th>\n",
       "      <th>WMO_PRES_0</th>\n",
       "      <th>DIST2LAND_0</th>\n",
       "      <th>STORM_SPEED_0</th>\n",
       "      <th>...</th>\n",
       "      <th>DISPLACEMENT_LAT_GFSO_24_7</th>\n",
       "      <th>DISPLACEMENT_LON_GFSO_24_7</th>\n",
       "      <th>cat_basin_AN_7</th>\n",
       "      <th>cat_basin_EP_7</th>\n",
       "      <th>basin_NI_7</th>\n",
       "      <th>basin_SI_7</th>\n",
       "      <th>basin_SP_7</th>\n",
       "      <th>basin_WP_7</th>\n",
       "      <th>DISPLACEMENT_LAT_7</th>\n",
       "      <th>DISPLACEMENT_LON_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.529900</td>\n",
       "      <td>-45.957600</td>\n",
       "      <td>47.5</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>2111.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>-0.4225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.799999</td>\n",
       "      <td>-46.299999</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>2088.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1849</td>\n",
       "      <td>-0.4353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.972500</td>\n",
       "      <td>-46.657700</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>2054.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2151</td>\n",
       "      <td>-0.4647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>-47.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>-0.4998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.242300</td>\n",
       "      <td>-47.292702</td>\n",
       "      <td>57.5</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>-0.5002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25917</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.667500</td>\n",
       "      <td>-67.214996</td>\n",
       "      <td>40.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2359</td>\n",
       "      <td>0.5655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25918</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>38.599998</td>\n",
       "      <td>-67.099998</td>\n",
       "      <td>40.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.5493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25919</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>38.522301</td>\n",
       "      <td>-67.037697</td>\n",
       "      <td>40.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.5840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25920</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.400002</td>\n",
       "      <td>-66.900002</td>\n",
       "      <td>40.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.6744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25921</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>38.204800</td>\n",
       "      <td>-66.616402</td>\n",
       "      <td>40.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.6590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25922 rows × 1336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       YEAR_0  MONTH_0  DAY_0  HOUR_0      LAT_0      LON_0  WMO_WIND_0  \\\n",
       "0      2011.0      9.0   30.0    21.0  23.529900 -45.957600        47.5   \n",
       "1      2011.0     10.0    1.0     0.0  23.799999 -46.299999        50.0   \n",
       "2      2011.0     10.0    1.0     3.0  23.972500 -46.657700        52.5   \n",
       "3      2011.0     10.0    1.0     6.0  24.100000 -47.000000        55.0   \n",
       "4      2011.0     10.0    1.0     9.0  24.242300 -47.292702        57.5   \n",
       "...       ...      ...    ...     ...        ...        ...         ...   \n",
       "25917  2020.0      6.0   22.0     3.0  38.667500 -67.214996        40.0   \n",
       "25918  2020.0      6.0   22.0     6.0  38.599998 -67.099998        40.0   \n",
       "25919  2020.0      6.0   22.0     9.0  38.522301 -67.037697        40.0   \n",
       "25920  2020.0      6.0   22.0    12.0  38.400002 -66.900002        40.0   \n",
       "25921  2020.0      6.0   22.0    15.0  38.204800 -66.616402        40.0   \n",
       "\n",
       "       WMO_PRES_0  DIST2LAND_0  STORM_SPEED_0  ...  \\\n",
       "0          1004.0       2111.0            9.0  ...   \n",
       "1          1003.0       2088.0            8.0  ...   \n",
       "2          1001.0       2054.0            7.0  ...   \n",
       "3           999.0       2027.0            6.0  ...   \n",
       "4           998.0       2001.0            6.0  ...   \n",
       "...           ...          ...            ...  ...   \n",
       "25917       980.0        409.0            2.0  ...   \n",
       "25918       980.0        423.0            2.0  ...   \n",
       "25919       980.0        437.0            3.0  ...   \n",
       "25920       980.0        451.0            5.0  ...   \n",
       "25921       980.0        484.0            6.0  ...   \n",
       "\n",
       "       DISPLACEMENT_LAT_GFSO_24_7  DISPLACEMENT_LON_GFSO_24_7  cat_basin_AN_7  \\\n",
       "0                             NaN                         NaN             1.0   \n",
       "1                             NaN                         NaN             1.0   \n",
       "2                             NaN                         NaN             1.0   \n",
       "3                             NaN                         NaN             1.0   \n",
       "4                             NaN                         NaN             1.0   \n",
       "...                           ...                         ...             ...   \n",
       "25917                         NaN                         NaN             1.0   \n",
       "25918                         NaN                         NaN             1.0   \n",
       "25919                         NaN                         NaN             1.0   \n",
       "25920                         NaN                         NaN             1.0   \n",
       "25921                         NaN                         NaN             1.0   \n",
       "\n",
       "       cat_basin_EP_7  basin_NI_7  basin_SI_7  basin_SP_7  basin_WP_7  \\\n",
       "0                 0.0         0.0         0.0         0.0         0.0   \n",
       "1                 0.0         0.0         0.0         0.0         0.0   \n",
       "2                 0.0         0.0         0.0         0.0         0.0   \n",
       "3                 0.0         0.0         0.0         0.0         0.0   \n",
       "4                 0.0         0.0         0.0         0.0         0.0   \n",
       "...               ...         ...         ...         ...         ...   \n",
       "25917             0.0         0.0         0.0         0.0         0.0   \n",
       "25918             0.0         0.0         0.0         0.0         0.0   \n",
       "25919             0.0         0.0         0.0         0.0         0.0   \n",
       "25920             0.0         0.0         0.0         0.0         0.0   \n",
       "25921             0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "       DISPLACEMENT_LAT_7  DISPLACEMENT_LON_7  \n",
       "0                  0.2076             -0.4225  \n",
       "1                  0.1849             -0.4353  \n",
       "2                  0.2151             -0.4647  \n",
       "3                  0.3000             -0.4998  \n",
       "4                  0.3000             -0.5002  \n",
       "...                   ...                 ...  \n",
       "25917              0.2359              0.5655  \n",
       "25918              0.2495              0.5493  \n",
       "25919              0.2505              0.5840  \n",
       "25920              0.2077              0.6744  \n",
       "25921              0.1923              0.6590  \n",
       "\n",
       "[25922 rows x 1336 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26800, 740)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EP:\n",
    "We beat: SHIP, HWRF, GFSO, AEMN, DSHP, (GFDL, UKXI, CMC)\n",
    "We lose: OFCL, FSSE\n",
    "\n",
    "AN:\n",
    "We beat: SHIP, HWRF, GFSO, AEMN, GFDL, DSHP, (UKXI, CMC) \n",
    "We lose: HWRF, OFCL, FSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.07, max_delta_step=0, max_depth=8,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=150, n_jobs=0, num_parallel_tree=1,\n",
       "       objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
       "       tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = X_train_total\n",
    "    #test = X_test_total\n",
    "tgt_train = tgt_intensity_train\n",
    "\n",
    "xgb_total = XGBRegressor(max_depth=8, n_estimators=150, learning_rate=0.07, subsample=0.8, min_child_weight=1)\n",
    "xgb_total.fit(train, tgt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps 1308\n",
      "MAE intensity basin AN Hurricast :  9.79 with std  13.03\n",
      "MAE intensity basin AN Official Forecast HWRF :  9.38 with std  12.77\n",
      "MAE intensity basin AN Official Forecast SHIP :  10.36 with std  14.24\n",
      "Percentage of missed intensification > 20kn Hurricast:  12.0\n",
      "Percentage of missed intensification > 20kn Official ForecastHWRF :  10.86\n",
      "Percentage of missed intensification > 20kn Official Forecast 2SHIP :  12.84\n",
      "\n",
      "MAE intensity basin AN Hurricast last 1000 :  10.28\n",
      "MAE intensity basin AN Official Forecast HWRF :  9.38\n"
     ]
    }
   ],
   "source": [
    "compare_perf_intensity(xgb_total = xgb_total, basin='AN', forecast='HWRF', mode='vmax', forecast2 = 'SHIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  208\n",
      "Year  2019  MAE intensity basin AN Hurricast trained full:  10.26 with std  13.18\n",
      "Year  2019  MAE intensity basin AN Hurricast trained until 2012:  10.26 with std  13.18\n",
      "Year  2019  MAE intensity basin AN Official Forecast HWRF :  9.89 with std  12.68\n",
      "Year  2019  MAE intensity basin AN Official Forecast SHIP :  10.89 with std  14.22\n",
      "Year  2019  MAE intensity basin AN Official Forecast OFCL :  8.55 with std  11.53\n",
      "Year  2019  MAE intensity basin AN Official Forecast FSSE :  8.73 with std  11.42\n"
     ]
    }
   ],
   "source": [
    "compare_perf_intensity_per_year_4cast(dict = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[]}, xgb_total = xgb_total, xgb_tot = xgb_total, basin='AN', forecast='HWRF', mode='vmax', forecast2 = 'SHIP', forecast3 = 'OFCL', forecast4 = 'FSSE', year = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  0\n",
      "\n",
      " No forecasts for year  2012\n",
      "Total number of steps for comparison:  6\n",
      "Year  2013  MAE intensity basin EP Hurricast trained full:  20.71 with std  7.0\n",
      "Year  2013  MAE intensity basin EP Hurricast trained until 2012:  22.98 with std  9.24\n",
      "Year  2013  MAE intensity basin EP Official Forecast HWRF :  11.33 with std  9.29\n",
      "Year  2013  MAE intensity basin EP Official Forecast SHIP :  27.0 with std  13.22\n",
      "Year  2013  MAE intensity basin EP Official Forecast OFCL :  10.0 with std  13.74\n",
      "Year  2013  MAE intensity basin EP Official Forecast FSSE :  10.67 with std  12.78\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  273\n",
      "Year  2014  MAE intensity basin EP Hurricast trained full:  10.24 with std  13.87\n",
      "Year  2014  MAE intensity basin EP Hurricast trained until 2012:  10.18 with std  13.75\n",
      "Year  2014  MAE intensity basin EP Official Forecast HWRF :  11.41 with std  14.57\n",
      "Year  2014  MAE intensity basin EP Official Forecast SHIP :  12.02 with std  15.44\n",
      "Year  2014  MAE intensity basin EP Official Forecast OFCL :  9.73 with std  13.45\n",
      "Year  2014  MAE intensity basin EP Official Forecast FSSE :  10.01 with std  13.07\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  274\n",
      "Year  2015  MAE intensity basin EP Hurricast trained full:  11.46 with std  15.62\n",
      "Year  2015  MAE intensity basin EP Hurricast trained until 2012:  11.94 with std  16.04\n",
      "Year  2015  MAE intensity basin EP Official Forecast HWRF :  12.28 with std  17.5\n",
      "Year  2015  MAE intensity basin EP Official Forecast SHIP :  13.06 with std  17.95\n",
      "Year  2015  MAE intensity basin EP Official Forecast OFCL :  10.89 with std  15.91\n",
      "Year  2015  MAE intensity basin EP Official Forecast FSSE :  10.66 with std  14.73\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  315\n",
      "Year  2016  MAE intensity basin EP Hurricast trained full:  9.77 with std  13.45\n",
      "Year  2016  MAE intensity basin EP Hurricast trained until 2012:  9.86 with std  13.31\n",
      "Year  2016  MAE intensity basin EP Official Forecast HWRF :  10.38 with std  13.74\n",
      "Year  2016  MAE intensity basin EP Official Forecast SHIP :  9.87 with std  12.84\n",
      "Year  2016  MAE intensity basin EP Official Forecast OFCL :  8.72 with std  11.89\n",
      "Year  2016  MAE intensity basin EP Official Forecast FSSE :  8.51 with std  11.53\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  167\n",
      "Year  2017  MAE intensity basin EP Hurricast trained full:  9.11 with std  12.97\n",
      "Year  2017  MAE intensity basin EP Hurricast trained until 2012:  9.07 with std  13.08\n",
      "Year  2017  MAE intensity basin EP Official Forecast HWRF :  7.87 with std  11.61\n",
      "Year  2017  MAE intensity basin EP Official Forecast SHIP :  9.97 with std  13.0\n",
      "Year  2017  MAE intensity basin EP Official Forecast OFCL :  8.14 with std  11.67\n",
      "Year  2017  MAE intensity basin EP Official Forecast FSSE :  7.92 with std  10.67\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  357\n",
      "Year  2018  MAE intensity basin EP Hurricast trained full:  12.19 with std  15.76\n",
      "Year  2018  MAE intensity basin EP Hurricast trained until 2012:  12.7 with std  16.45\n",
      "Year  2018  MAE intensity basin EP Official Forecast HWRF :  10.92 with std  14.83\n",
      "Year  2018  MAE intensity basin EP Official Forecast SHIP :  12.92 with std  15.86\n",
      "Year  2018  MAE intensity basin EP Official Forecast OFCL :  10.18 with std  13.44\n",
      "Year  2018  MAE intensity basin EP Official Forecast FSSE :  9.82 with std  12.65\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  40\n",
      "Year  2019  MAE intensity basin EP Hurricast trained full:  4.18 with std  4.04\n",
      "Year  2019  MAE intensity basin EP Hurricast trained until 2012:  4.16 with std  4.4\n",
      "Year  2019  MAE intensity basin EP Official Forecast HWRF :  21.71 with std  21.95\n",
      "Year  2019  MAE intensity basin EP Official Forecast SHIP :  25.58 with std  21.77\n",
      "Year  2019  MAE intensity basin EP Official Forecast OFCL :  26.6 with std  23.02\n",
      "Year  2019  MAE intensity basin EP Official Forecast FSSE :  24.89 with std  20.87\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAES_2012': [22.98, 10.18, 11.94, 9.86, 9.07, 12.7, 4.16],\n",
       " 'MAES_FSSE': [10.67, 10.01, 10.66, 8.51, 7.92, 9.82, 24.89],\n",
       " 'MAES_HWRF': [27.0, 12.02, 13.06, 9.87, 9.97, 12.92, 25.58],\n",
       " 'MAES_OFCL': [10.0, 9.73, 10.89, 8.72, 8.14, 10.18, 26.6],\n",
       " 'MAES_SHIP': [11.33, 11.41, 12.28, 10.38, 7.87, 10.92, 21.71],\n",
       " 'MAEs_full': [20.71, 10.24, 11.46, 9.77, 9.11, 12.19, 4.18],\n",
       " 'num_samples': [6, 273, 274, 315, 167, 357, 40],\n",
       " 'std_2012': [9.24, 13.75, 16.04, 13.31, 13.08, 16.45, 4.4],\n",
       " 'std_FSSE': [12.78, 13.07, 14.73, 11.53, 10.67, 12.65, 20.87],\n",
       " 'std_HWRF': [13.22, 15.44, 17.95, 12.84, 13.0, 15.86, 21.77],\n",
       " 'std_OFCL': [13.74, 13.45, 15.91, 11.89, 11.67, 13.44, 23.02],\n",
       " 'std_SHIP': [9.29, 14.57, 17.5, 13.74, 11.61, 14.83, 21.95],\n",
       " 'std_full': [7.0, 13.87, 15.62, 13.45, 12.97, 15.76, 4.04],\n",
       " 'year': [2013, 2014, 2015, 2016, 2017, 2018, 2019]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xgb_intensity_all_years_full_train_4cast(forecast = 'HWRF', basin_only = False, sparse = False, max_depth = 8, n_estimators = 150, learning_rate = 0.07, subsample = 0.8, min_child_weight=5, basin = 'EP', forecast2 = 'SHIP', forecast3 = 'OFCL', forecast4 = 'FSSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps for comparison:  0\n",
      "\n",
      " No forecasts for year  2012\n",
      "Total number of steps for comparison:  6\n",
      "Year  2013  MAE intensity basin EP Hurricast trained full:  22.26 with std  8.79\n",
      "Year  2013  MAE intensity basin EP Hurricast trained until 2012:  21.47 with std  6.96\n",
      "Year  2013  MAE intensity basin EP Official Forecast HWRF :  11.33 with std  9.29\n",
      "Year  2013  MAE intensity basin EP Official Forecast SHIP :  27.0 with std  13.22\n",
      "Year  2013  MAE intensity basin EP Official Forecast OFCL :  10.0 with std  13.74\n",
      "Year  2013  MAE intensity basin EP Official Forecast FSSE :  10.67 with std  12.78\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  273\n",
      "Year  2014  MAE intensity basin EP Hurricast trained full:  10.14 with std  13.69\n",
      "Year  2014  MAE intensity basin EP Hurricast trained until 2012:  10.14 with std  13.79\n",
      "Year  2014  MAE intensity basin EP Official Forecast HWRF :  11.41 with std  14.57\n",
      "Year  2014  MAE intensity basin EP Official Forecast SHIP :  12.02 with std  15.44\n",
      "Year  2014  MAE intensity basin EP Official Forecast OFCL :  9.73 with std  13.45\n",
      "Year  2014  MAE intensity basin EP Official Forecast FSSE :  10.01 with std  13.07\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  274\n",
      "Year  2015  MAE intensity basin EP Hurricast trained full:  11.68 with std  15.87\n",
      "Year  2015  MAE intensity basin EP Hurricast trained until 2012:  11.69 with std  15.82\n",
      "Year  2015  MAE intensity basin EP Official Forecast HWRF :  12.28 with std  17.5\n",
      "Year  2015  MAE intensity basin EP Official Forecast SHIP :  13.06 with std  17.95\n",
      "Year  2015  MAE intensity basin EP Official Forecast OFCL :  10.89 with std  15.91\n",
      "Year  2015  MAE intensity basin EP Official Forecast FSSE :  10.66 with std  14.73\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  315\n",
      "Year  2016  MAE intensity basin EP Hurricast trained full:  9.58 with std  13.32\n",
      "Year  2016  MAE intensity basin EP Hurricast trained until 2012:  9.94 with std  13.39\n",
      "Year  2016  MAE intensity basin EP Official Forecast HWRF :  10.38 with std  13.74\n",
      "Year  2016  MAE intensity basin EP Official Forecast SHIP :  9.87 with std  12.84\n",
      "Year  2016  MAE intensity basin EP Official Forecast OFCL :  8.72 with std  11.89\n",
      "Year  2016  MAE intensity basin EP Official Forecast FSSE :  8.51 with std  11.53\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  167\n",
      "Year  2017  MAE intensity basin EP Hurricast trained full:  9.12 with std  12.79\n",
      "Year  2017  MAE intensity basin EP Hurricast trained until 2012:  9.04 with std  13.09\n",
      "Year  2017  MAE intensity basin EP Official Forecast HWRF :  7.87 with std  11.61\n",
      "Year  2017  MAE intensity basin EP Official Forecast SHIP :  9.97 with std  13.0\n",
      "Year  2017  MAE intensity basin EP Official Forecast OFCL :  8.14 with std  11.67\n",
      "Year  2017  MAE intensity basin EP Official Forecast FSSE :  7.92 with std  10.67\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  357\n",
      "Year  2018  MAE intensity basin EP Hurricast trained full:  12.21 with std  15.82\n",
      "Year  2018  MAE intensity basin EP Hurricast trained until 2012:  12.37 with std  16.1\n",
      "Year  2018  MAE intensity basin EP Official Forecast HWRF :  10.92 with std  14.83\n",
      "Year  2018  MAE intensity basin EP Official Forecast SHIP :  12.92 with std  15.86\n",
      "Year  2018  MAE intensity basin EP Official Forecast OFCL :  10.18 with std  13.44\n",
      "Year  2018  MAE intensity basin EP Official Forecast FSSE :  9.82 with std  12.65\n",
      "\n",
      "\n",
      "Total number of steps for comparison:  40\n",
      "Year  2019  MAE intensity basin EP Hurricast trained full:  4.02 with std  4.2\n",
      "Year  2019  MAE intensity basin EP Hurricast trained until 2012:  3.89 with std  4.25\n",
      "Year  2019  MAE intensity basin EP Official Forecast HWRF :  21.71 with std  21.95\n",
      "Year  2019  MAE intensity basin EP Official Forecast SHIP :  25.58 with std  21.77\n",
      "Year  2019  MAE intensity basin EP Official Forecast OFCL :  26.6 with std  23.02\n",
      "Year  2019  MAE intensity basin EP Official Forecast FSSE :  24.89 with std  20.87\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAES_2012': [21.47, 10.14, 11.69, 9.94, 9.04, 12.37, 3.89],\n",
       " 'MAES_FSSE': [10.67, 10.01, 10.66, 8.51, 7.92, 9.82, 24.89],\n",
       " 'MAES_HWRF': [27.0, 12.02, 13.06, 9.87, 9.97, 12.92, 25.58],\n",
       " 'MAES_OFCL': [10.0, 9.73, 10.89, 8.72, 8.14, 10.18, 26.6],\n",
       " 'MAES_SHIP': [11.33, 11.41, 12.28, 10.38, 7.87, 10.92, 21.71],\n",
       " 'MAEs_full': [22.26, 10.14, 11.68, 9.58, 9.12, 12.21, 4.02],\n",
       " 'num_samples': [6, 273, 274, 315, 167, 357, 40],\n",
       " 'std_2012': [6.96, 13.79, 15.82, 13.39, 13.09, 16.1, 4.25],\n",
       " 'std_FSSE': [12.78, 13.07, 14.73, 11.53, 10.67, 12.65, 20.87],\n",
       " 'std_HWRF': [13.22, 15.44, 17.95, 12.84, 13.0, 15.86, 21.77],\n",
       " 'std_OFCL': [13.74, 13.45, 15.91, 11.89, 11.67, 13.44, 23.02],\n",
       " 'std_SHIP': [9.29, 14.57, 17.5, 13.74, 11.61, 14.83, 21.95],\n",
       " 'std_full': [8.79, 13.69, 15.87, 13.32, 12.79, 15.82, 4.2],\n",
       " 'year': [2013, 2014, 2015, 2016, 2017, 2018, 2019]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xgb_intensity_all_years_full_train_4cast(forecast = 'HWRF', basin_only = False, sparse = False, max_depth = 8, n_estimators = 120, learning_rate = 0.07, subsample = 0.8, min_child_weight=1, basin = 'EP', forecast2 = 'SHIP', forecast3 = 'OFCL', forecast4 = 'FSSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_intensity_all_years_full_train(forecast2 = None, basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'HWRF'):\n",
    "    train = X_train_total\n",
    "    #test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    if sparse:\n",
    "        train = X_train_total_sparse_x\n",
    "        #test = X_test_total_sparse_x\n",
    "    if basin_only:\n",
    "        train = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        tgt_train = tgt_intensity_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_total_1 = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total_1.fit(train, tgt_train)\n",
    "    dict = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[]}\n",
    "    for year in range(2012, 2020):\n",
    "            index = X_test_baseline.loc[X_test_baseline['YEAR'] < year].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "            X_test_to_train = X_test_total[index]\n",
    "            train = np.concatenate((X_train_total, X_test_to_train), axis = 0)\n",
    "            tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[index]), axis = 0)\n",
    "            xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                     subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_total.fit(train, tgt_train)\n",
    "            compare_perf_intensity_per_year(dict = dict, xgb_tot = xgb_total_1, forecast2 = forecast2, xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', year = year)\n",
    "            print(\"\\n\")\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-eefc5c1222b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_xgb_intensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EMXI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasin_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.07\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'OFCL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-cab3eb48c918>\u001b[0m in \u001b[0;36mtrain_xgb_intensity\u001b[0;34m(forecast, basin_only, sparse, max_depth, n_estimators, learning_rate, subsample, min_child_weight, basin, forecast2)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mxgb_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mxgb_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mcompare_perf_intensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbasin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecast2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m#train_xgb_track(n_estimators = 90, max_depth = 7, learning_rate = 0.12, subsample = 0.7, min_child_weight = 5, basin = 'AN', forecast = 'HWRF') 82.14 and 117.26\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-cab3eb48c918>\u001b[0m in \u001b[0;36mcompare_perf_intensity\u001b[0;34m(xgb_total, basin, forecast, last_storms, mode, forecast2)\u001b[0m\n\u001b[1;32m    178\u001b[0m                                                      \u001b[0;31m#xgb.predict(X_test_withBASELINE) * std_ + mean_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timesteps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAE intensity basin \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbasin\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Hurricast : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"with std \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         print(\"MAE intensity basin \" + basin + \" Official Forecast \"+ forecast + \" : \",\n\u001b[1;32m    182\u001b[0m               np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 170\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2020a/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 582\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "train_xgb_intensity(forecast = 'EMXI', basin_only = False, sparse = False, max_depth = 8, n_estimators = 150, learning_rate = 0.07, subsample = 0.8, min_child_weight=1, basin = 'EP', forecast2 = 'OFCL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train_total\n",
    "train_y = X_train_total\n",
    "test_x = X_test_total\n",
    "test_y = X_test_total\n",
    "tgt_train = tgt_displacement_train\n",
    "\n",
    "xgb_x = XGBRegressor(max_depth=7, n_estimators=90, learning_rate=0.07, subsample=0.7, min_child_weight=5)\n",
    "xgb_x.fit(train_x, tgt_train[:, 0])\n",
    "xgb_y = XGBRegressor(max_depth=7, n_estimators=90, learning_rate=0.07, subsample=0.7, min_child_weight=5)\n",
    "xgb_y.fit(train_y, tgt_train[:, 1])\n",
    "DLATS_PRED = np.array(xgb_x.predict(test_x)) * std_dx + mean_dx\n",
    "DLONS_PRED = np.array(xgb_y.predict(test_y)) * std_dy + mean_dy\n",
    "LATS_PRED_ = X_test['LAT_7'] + DLATS_PRED\n",
    "LONS_PRED_ = X_test['LON_7'] + DLONS_PRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        42.100052\n",
       "1        42.950531\n",
       "2        43.727261\n",
       "3        44.291496\n",
       "4        11.833095\n",
       "           ...    \n",
       "26795    41.138153\n",
       "26796    41.615891\n",
       "26797    41.676640\n",
       "26798    41.337860\n",
       "26799    41.493984\n",
       "Name: LAT_7, Length: 26800, dtype: float32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LATS_PRED_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        42.151398\n",
       "1        42.899998\n",
       "2        43.847599\n",
       "3        44.899998\n",
       "4        12.634900\n",
       "           ...    \n",
       "26795    40.200001\n",
       "26796    40.669701\n",
       "26797    41.200001\n",
       "26798    41.635597\n",
       "26799    42.000000\n",
       "Name: LAT_7, Length: 26800, dtype: float32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LATS_TEST_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP: Intensity\n",
    "We beat: HWRF, SHIP, GFSO, AEMN, DSHP, (GFDL, UKXI, CMC)\n",
    "We lose: OFCL, FSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP: Track\n",
    "We beat: GFDL, CMC, CLP5\n",
    "We lose: FSSE, OFCL, SHIP, HWRF, GFSO, AEMN, DSHP (UKXI)\n",
    "    \n",
    "AN: Track\n",
    "We beat: GFDL, CMC, CLP5\n",
    "We lose: FSSE, OFCL, SHIP, HWRF, GFSO, AEMN, DSHP (UKXI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_perf_track(basin='EP', forecast='SHIP', forecast2 = 'HWRF', LATS_PRED_=LATS_PRED_, LONS_PRED_=LONS_PRED_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_track(basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'SHIP', forecast2 = None):\n",
    "    train_x = X_train_total\n",
    "    train_y = X_train_total\n",
    "    test_x = X_test_total\n",
    "    test_y = X_test_total\n",
    "    tgt_train = tgt_displacement_train\n",
    "    if sparse:\n",
    "        train_x, train_y = X_train_total_sparse_x, X_train_total_sparse_y\n",
    "        test_x, test_y = X_test_total_sparse_x, X_test_total_sparse_y\n",
    "    if basin_only:\n",
    "        train_x = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        train_y = train_x\n",
    "        tgt_train = tgt_displacement_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_x.fit(train_x, tgt_train[:, 0])\n",
    "    xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_y.fit(train_y, tgt_train[:, 1])\n",
    "    DLATS_PRED = np.array(xgb_x.predict(test_x)) * std_dx + mean_dx\n",
    "    DLONS_PRED = np.array(xgb_y.predict(test_y)) * std_dy + mean_dy\n",
    "    LATS_PRED_ = X_test['LAT_7'] + DLATS_PRED\n",
    "    LONS_PRED_ = X_test['LON_7'] + DLONS_PRED\n",
    "    compare_perf_track(basin=basin, forecast=forecast, forecast2 = forecast2, LATS_PRED_=LATS_PRED_, LONS_PRED_=LONS_PRED_)\n",
    "\n",
    "def train_xgb_track_all_years(use_forecast = False, basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'SHIP', forecast2 = None):\n",
    "    train_x = X_train_total\n",
    "    train_y = X_train_total\n",
    "    test_x = X_test_total\n",
    "    test_y = X_test_total\n",
    "    tgt_train = tgt_displacement_train\n",
    "    if sparse:\n",
    "        train_x, train_y = X_train_total_sparse_x, X_train_total_sparse_y\n",
    "        test_x, test_y = X_test_total_sparse_x, X_test_total_sparse_y\n",
    "    if basin_only:\n",
    "        train_x = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        train_y = train_x\n",
    "        tgt_train = tgt_displacement_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    if use_forecast:\n",
    "        train_for = X_train_forecasts\n",
    "        tgt_train_for = tgt_train_dis_forecasts\n",
    "        test_for = X_test_forecasts\n",
    "        xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "        xgb_x.fit(train_for, tgt_train_for[:, 0])\n",
    "        xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "        xgb_y.fit(train_for, tgt_train_for[:, 1])\n",
    "        DLATS_PRED = np.array(xgb_x.predict(X_new)) * std_dx + mean_dx\n",
    "        DLONS_PRED = np.array(xgb_y.predict(X_new)) * std_dy + mean_dy\n",
    "    else:\n",
    "        xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "        xgb_x.fit(train_x, tgt_train[:, 0])\n",
    "        xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "        xgb_y.fit(train_y, tgt_train[:, 1])\n",
    "        DLATS_PRED = np.array(xgb_x.predict(test_x)) * std_dx + mean_dx\n",
    "        DLONS_PRED = np.array(xgb_y.predict(test_y)) * std_dy + mean_dy\n",
    "    LATS_PRED_2012 = X_test['LAT_7'] + DLATS_PRED\n",
    "    LONS_PRED_2012 = X_test['LON_7'] + DLONS_PRED\n",
    "    compare_perf_track(basin=basin, forecast=forecast, forecast2 = forecast2, LATS_PRED_=LATS_PRED_2012, LONS_PRED_=LONS_PRED_2012)\n",
    "    dict = {'year': [], 'num_samples': [], 'MAEs_full': [], 'std_full': [], 'MAES_2012': [], 'std_2012': [],\n",
    "            'MAES_SHIP': [], 'std_SHIP': [], 'MAES_HWRF': [], 'std_HWRF': []}\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            index = X_test_baseline.loc[\n",
    "                X_test_baseline['YEAR'] < year].index\n",
    "            X_test_to_train = X_test_total[index]\n",
    "            train = np.concatenate((X_train_total, X_test_to_train), axis=0)\n",
    "            tgt_train = np.concatenate((tgt_displacement_train, tgt_displacement_test[index]), axis=0)\n",
    "            xgb_x = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                 subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_x.fit(train, tgt_train[:, 0])\n",
    "            xgb_y = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                 subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_y.fit(train, tgt_train[:, 1])\n",
    "            DLATS_PRED = np.array(xgb_x.predict(test_x)) * std_dx + mean_dx\n",
    "            DLONS_PRED = np.array(xgb_y.predict(test_y)) * std_dy + mean_dy\n",
    "            LATS_PRED_ = X_test['LAT_7'] + DLATS_PRED\n",
    "            LONS_PRED_ = X_test['LON_7'] + DLONS_PRED\n",
    "            compare_perf_track_per_year(dict, LATS_PRED_, LONS_PRED_, LATS_PRED_2012, LONS_PRED_2012, forecast=forecast,\n",
    "                                            forecast2=forecast2, basin=basin,  year=year)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "    return dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_xgb_intensity(forecast = 'SHIP', basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast2 = None):\n",
    "    train = X_train_total\n",
    "    #test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    if sparse:\n",
    "        train = X_train_total_sparse_x\n",
    "        test = X_test_total_sparse_x\n",
    "    if basin_only:\n",
    "        train = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        tgt_train = tgt_intensity_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total.fit(train, tgt_train)\n",
    "    compare_perf_intensity(xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', forecast2 = forecast2)\n",
    "\n",
    "#train_xgb_track(n_estimators = 90, max_depth = 7, learning_rate = 0.12, subsample = 0.7, min_child_weight = 5, basin = 'AN', forecast = 'HWRF') 82.14 and 117.26\n",
    "#train_xgb_track(n_estimators = 90, max_depth = 7, learning_rate = 0.1, subsample = 0.7, min_child_weight = 5, basin = 'EP', forecast = 'HWRF')\n",
    "\n",
    "LATS_TEST = X_test['LAT_7'] + np.array(tgt_displacement_test[:,0])*std_dx+mean_dx\n",
    "LONS_TEST = X_test['LON_7'] + np.array(tgt_displacement_test[:,1])*std_dy+mean_dy\n",
    "\n",
    "def compare_perf_track(LATS_PRED_, LONS_PRED_, basin = 'AN', forecast = 'SHIP', forecast2 = None):\n",
    "    mode = 'lat'\n",
    "    if forecast2 != None:\n",
    "        index = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline[\n",
    "                                                                             'cat_basin_' + basin + '_0'] == 1].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        baseline_ = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        LATS_BASE_2 = np.array(baseline_[forecast2 + '_24_lat_7'])\n",
    "        LONS_BASE_2 = np.array(baseline_[forecast2 + '_24_lon_7'])\n",
    "    else:\n",
    "        index = X_test_baseline.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1].index#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "    LATS_TEST_ = X_test['LAT_7'] + np.array(tgt_displacement_test[:, 0])*std_dx+mean_dx\n",
    "    LONS_TEST_ = X_test['LON_7'] + np.array(tgt_displacement_test[:, 1])*std_dy+mean_dy\n",
    "    baseline_1_x = baseline_[forecast + '_24_'+mode+'_7']\n",
    "    baseline_1_y = baseline_[forecast + '_24_lon_7']\n",
    "    LATS_BASE = np.array(baseline_1_x)\n",
    "    LONS_BASE = np.array(baseline_1_y)\n",
    "    LATS_TEST_ = np.array(LATS_TEST_[index])\n",
    "    LONS_TEST_ = np.array(LONS_TEST_[index])\n",
    "    LATS_PRED_ = np.array(LATS_PRED_[index])\n",
    "    LONS_PRED_ = np.array(LONS_PRED_[index])\n",
    "    d_km_baseline = np.zeros(len(LATS_BASE))\n",
    "    d_km_baseline2 = np.zeros(len(LATS_BASE))\n",
    "    d_km_pred = np.zeros(len(LONS_PRED_))\n",
    "    for i in range(len(LATS_BASE)):\n",
    "        d_km_baseline[i] = get_distance_km(LONS_BASE[i], LATS_BASE[i], LONS_TEST_[i], LATS_TEST_[i])\n",
    "        d_km_pred[i] = get_distance_km(LONS_PRED_[i], LATS_PRED_[i], LONS_TEST_[i], LATS_TEST_[i])\n",
    "        if forecast2 != None:\n",
    "            d_km_baseline2[i] = get_distance_km(LONS_BASE_2[i], LATS_BASE_2[i], LONS_TEST_[i], LATS_TEST_[i])\n",
    "    print(\"Number of timesteps:\", len(LATS_BASE))\n",
    "    print(basin, 'Model | MAE | std')\n",
    "    print(forecast, np.around(d_km_baseline.mean(), decimals = 2), np.around(d_km_baseline.std(), decimals = 2))\n",
    "    print(str(forecast2), np.around(d_km_baseline2.mean(), decimals = 2), np.around(d_km_baseline2.std(), decimals = 2))\n",
    "    print(\"Hurricast\", np.around(d_km_pred.mean(), decimals = 2), np.around(d_km_pred.std(), decimals = 2))\n",
    "    print(\"\\nModel | Number of Busts > 200km | Percentage Bust\")\n",
    "    print(forecast, sum(d_km_baseline > 200), np.around(sum(d_km_baseline > 200)*100/len(LATS_BASE), decimals = 2))\n",
    "    print(str(forecast2), sum(d_km_baseline2 > 200), np.around(sum(d_km_baseline2 > 200)*100/len(LATS_BASE), decimals =2))\n",
    "    print(\"Hurricast\", sum(d_km_pred > 200), np.around(sum(d_km_pred > 200)*100/len(LATS_BASE), decimals = 2))\n",
    "\n",
    "\n",
    "\n",
    "def compare_perf_intensity(xgb_total, basin = 'AN', forecast = 'SHIP', last_storms = 1000, mode = 'vmax', forecast2 = 'HWRF'):\n",
    "    if forecast2 != None:\n",
    "        index = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline[\n",
    "                                                                             'cat_basin_' + basin + '_0'] == 1].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        # X_test_withBASELINE = X_test.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_ = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    else:\n",
    "        index = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline[\n",
    "                                                                             'cat_basin_' + basin + '_0'] == 1].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        # X_test_withBASELINE = X_test.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_ = X_test_baseline.loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "    X_test_withBASELINE_total = X_test_total[index]\n",
    "    baseline_1 = baseline_[forecast + '_24_' + mode + '_7']\n",
    "    if mode == 'vmax':\n",
    "        tgt_ = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "        preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        #print(\"MAE intensity basin \" + basin + \" X stat vs \"+ forecast + \" : \", mean_absolute_error(tgt_intensity_test_withBASELINE * std_ + mean_,\n",
    "                                                     #xgb.predict(X_test_withBASELINE) * std_ + mean_))\n",
    "        print(\"Timesteps\", len(tgt_))\n",
    "        print(\"MAE intensity basin \" + basin + \" Hurricast : \", np.around(mean_absolute_error(tgt_, preds), decimals = 2), \"with std \", np.around(np.std(tgt_ - preds), decimals=2))\n",
    "        print(\"MAE intensity basin \" + basin + \" Official Forecast \"+ forecast + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "        print(\"MAE intensity basin \" + basin + \" Official Forecast \" + str(forecast2) + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_, baseline_2), decimals=2), \"with std \",\n",
    "              np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "        print(\"Percentage of missed intensification > 20kn Hurricast: \", np.around(sum(abs(tgt_ - preds) > 20)/len(preds) * 100, decimals = 2))\n",
    "        print(\"Percentage of missed intensification > 20kn Official Forecast\"+ forecast + \" : \", np.around(sum(abs(tgt_ - baseline_1) > 20) / len(baseline_1) * 100, decimals =2))\n",
    "        print(\"Percentage of missed intensification > 20kn Official Forecast 2\"+ str(forecast2) + \" : \", np.around(sum(abs(tgt_ - baseline_2) > 20) / len(baseline_2) * 100, decimals =2))\n",
    "        print(\"\\nMAE intensity basin \" + basin + \" Hurricast last\", last_storms, \": \", np.around(mean_absolute_error(tgt_[-last_storms:], preds[-last_storms:]), decimals=2))\n",
    "        print(\"MAE intensity basin \" + basin + \" Official Forecast \" + forecast + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_[-last_storms:], baseline_1[-last_storms:]), decimals=2))\n",
    "\n",
    "\n",
    "#train_xgb_intensity(forecast = 'SHIP', basin = 'EP', max_depth=8, n_estimators = 120, learning_rate = 0.07, subsample = 0.8, min_child_weight = 1)\n",
    "#train_xgb_intensity(forecast = 'SHIP', basin = 'AN', max_depth=8, n_estimators = 150, learning_rate = 0.07, subsample = 0.8, min_child_weight = 1, forecast2 = 'HWRF')\n",
    "\n",
    "\n",
    "def compare_perf_intensity_per_year(dict, xgb_tot, xgb_total, year, forecast2, basin = 'AN', forecast = 'HWRF', mode = 'vmax'):\n",
    "    if forecast2 != None:\n",
    "        index = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        # X_test_withBASELINE = X_test.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    else:\n",
    "        index = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1].index#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        #X_test_withBASELINE = X_test.loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "    X_test_withBASELINE_total = X_test_total[index]\n",
    "    baseline_1 = baseline_[forecast + '_24_'+mode+'_7']\n",
    "    if mode == 'vmax':\n",
    "        tgt_ = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "        print(\"Total number of steps for comparison: \", len(tgt_))\n",
    "        preds_1 = xgb_tot.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        #print(\"MAE intensity basin \" + basin + \" X stat vs \"+ forecast + \" : \", mean_absolute_error(tgt_intensity_test_withBASELINE * std_ + mean_,\n",
    "                                                     #xgb.predict(X_test_withBASELINE) * std_ + mean_))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Hurricast trained full: \", np.around(mean_absolute_error(tgt_, preds), decimals = 2), \"with std \", np.around(np.std(tgt_ - preds), decimals=2))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Hurricast trained until 2012: \", np.around(mean_absolute_error(tgt_, preds_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - preds_1), decimals=2))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Official Forecast \"+ forecast + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "        if forecast2 != None:\n",
    "            print(\"Year \", year, \" MAE intensity basin \" + basin + \" Official Forecast \" + forecast2 + \" : \",\n",
    "                np.around(mean_absolute_error(tgt_, baseline_2), decimals=2), \"with std \",\n",
    "                np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "        append_dict_intensity(dict, tgt_, preds, preds_1, baseline_1, baseline_2, year)\n",
    "        #print(\"Year \", year, \" Percentage of missed intensification > 20kn Hurricast: \", np.around(sum(tgt_ - preds > 20)/len(preds) * 100, decimals = 2))\n",
    "        #print(\"Year \", year, \" Percentage of missed intensification > 20kn Official Forecast: \", np.around(sum(tgt_ - baseline_1 > 20) / len(baseline_1) * 100, decimals =2))\n",
    "\n",
    "\n",
    "def compare_perf_track_per_year(dict, LATS_PRED_, LONS_PRED_, LATS_PRED_2012, LONS_PRED_2012, year, basin = 'AN', forecast = 'SHIP', forecast2 = None):\n",
    "    mode = 'lat'\n",
    "    if forecast2 != None:\n",
    "        index = X_test_baseline.loc[X_test_baseline['YEAR'] == year].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline[\n",
    "                                                                             'cat_basin_' + basin + '_0'] == 1].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR'] == year].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "        LATS_BASE_2 = np.array(baseline_[forecast2 + '_24_lat_7'])\n",
    "        LONS_BASE_2 = np.array(baseline_[forecast2 + '_24_lon_7'])\n",
    "    else:\n",
    "        index = X_test_baseline.loc[X_test_baseline['YEAR'] == year].loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1].index#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "        baseline_ = X_test_baseline.loc[X_test_baseline['YEAR'] == year].loc[X_test_baseline[forecast + '_24_'+mode+'_7'] > -320].loc[X_test_baseline['cat_basin_'+basin+'_0'] == 1]#.loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0]\n",
    "    LATS_TEST_ = X_test['LAT_7'] + np.array(tgt_displacement_test[:, 0])*std_dx+mean_dx\n",
    "    LONS_TEST_ = X_test['LON_7'] + np.array(tgt_displacement_test[:, 1])*std_dy+mean_dy\n",
    "    baseline_1_x = baseline_[forecast + '_24_'+mode+'_7']\n",
    "    baseline_1_y = baseline_[forecast + '_24_lon_7']\n",
    "    LATS_BASE = np.array(baseline_1_x)\n",
    "    LONS_BASE = np.array(baseline_1_y)\n",
    "    LATS_TEST_ = np.array(LATS_TEST_[index])\n",
    "    LONS_TEST_ = np.array(LONS_TEST_[index])\n",
    "    LATS_PRED_ = np.array(LATS_PRED_[index])\n",
    "    LONS_PRED_ = np.array(LONS_PRED_[index])\n",
    "    LATS_PRED_2012 = np.array(LATS_PRED_2012[index])\n",
    "    LONS_PRED_2012 = np.array(LONS_PRED_2012[index])\n",
    "    d_km_baseline = np.zeros(len(LATS_BASE))\n",
    "    d_km_baseline2 = np.zeros(len(LATS_BASE))\n",
    "    d_km_pred = np.zeros(len(LONS_PRED_))\n",
    "    d_km_pred_2012 = np.zeros(len(LONS_PRED_))\n",
    "    for i in range(len(LATS_BASE)):\n",
    "        d_km_baseline[i] = get_distance_km(LONS_BASE[i], LATS_BASE[i], LONS_TEST_[i], LATS_TEST_[i])\n",
    "        d_km_pred[i] = get_distance_km(LONS_PRED_[i], LATS_PRED_[i], LONS_TEST_[i], LATS_TEST_[i])\n",
    "        d_km_pred_2012[i] = get_distance_km(LONS_PRED_2012[i], LATS_PRED_2012[i], LONS_TEST_[i], LATS_TEST_[i])\n",
    "        if forecast2 != None:\n",
    "            d_km_baseline2[i] = get_distance_km(LONS_BASE_2[i], LATS_BASE_2[i], LONS_TEST_[i], LATS_TEST_[i])\n",
    "    print(\"Year\", year, \"Number of timesteps:\", len(LATS_BASE))\n",
    "    print(basin, 'Model | MAE | std')\n",
    "    print(forecast, np.around(d_km_baseline.mean(), decimals = 2), np.around(d_km_baseline.std(), decimals = 2))\n",
    "    print(str(forecast2), np.around(d_km_baseline2.mean(), decimals = 2), np.around(d_km_baseline2.std(), decimals = 2))\n",
    "    print(\"Hurricast Max Data\", np.around(d_km_pred.mean(), decimals = 2), np.around(d_km_pred.std(), decimals = 2))\n",
    "    print(\"Hurricast Until 2012\", np.around(d_km_pred_2012.mean(), decimals=2), np.around(d_km_pred_2012.std(), decimals=2))\n",
    "    print(\"\\nModel | Number of Busts > 200km | Percentage Bust\")\n",
    "    print(forecast, sum(d_km_baseline > 200), np.around(sum(d_km_baseline > 200)*100/len(LATS_BASE), decimals = 2))\n",
    "    print(str(forecast2), sum(d_km_baseline2 > 200), np.around(sum(d_km_baseline2 > 200)*100/len(LATS_BASE), decimals =2))\n",
    "    print(\"Hurricast\", sum(d_km_pred > 200), np.around(sum(d_km_pred > 200)*100/len(LATS_BASE), decimals = 2))\n",
    "    append_dict_track(dict, d_km_baseline, d_km_baseline2, d_km_pred, d_km_pred_2012, year)\n",
    "\n",
    "def append_dict_track(dict, d_km_baseline, d_km_baseline2, d_km_pred, d_km_pred_2012, year):\n",
    "    dict['year'].append(year)\n",
    "    dict['num_samples'].append(len(d_km_baseline))\n",
    "    dict['MAEs_full'].append(np.around(d_km_pred.mean(), decimals = 2))\n",
    "    dict['std_full'].append(np.around(d_km_pred.std(), decimals = 2))\n",
    "    dict['MAES_2012'].append(np.around(d_km_pred_2012.mean(), decimals = 2))\n",
    "    dict['std_2012'].append(np.around(d_km_pred_2012.std(), decimals=2))\n",
    "    dict['MAES_SHIP'].append(np.around(d_km_baseline.mean(), decimals = 2))\n",
    "    dict['std_SHIP'].append(np.around(d_km_baseline.std(), decimals = 2))\n",
    "    dict['MAES_HWRF'].append(np.around(d_km_baseline2.mean(), decimals =2))\n",
    "    dict['std_HWRF'].append(np.around(d_km_baseline2.std(), decimals=2))\n",
    "\n",
    "def append_dict_intensity(dict, tgt_, preds, preds_1, baseline_1, baseline_2, year):\n",
    "    dict['year'].append(year)\n",
    "    dict['num_samples'].append(len(tgt_))\n",
    "    dict['MAEs_full'].append(np.around(mean_absolute_error(tgt_, preds), decimals = 2))\n",
    "    dict['std_full'].append(np.around(np.std(tgt_ - preds), decimals = 2))\n",
    "    dict['MAES_2012'].append(np.around(mean_absolute_error(tgt_, preds_1), decimals = 2))\n",
    "    dict['std_2012'].append(np.around(np.std(tgt_ - preds_1), decimals=2))\n",
    "    dict['MAES_SHIP'].append(np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2))\n",
    "    dict['std_SHIP'].append(np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "    dict['MAES_HWRF'].append(np.around(mean_absolute_error(tgt_, baseline_2), decimals=2))\n",
    "    dict['std_HWRF'].append(np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "\n",
    "\n",
    "def train_xgb_intensity_all_years(forecast2 = None, basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'HWRF'):\n",
    "    train = X_train_total\n",
    "    #test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    if sparse:\n",
    "        train = X_train_total_sparse_x\n",
    "        #test = X_test_total_sparse_x\n",
    "    if basin_only:\n",
    "        train = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        tgt_train = tgt_intensity_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total.fit(train, tgt_train)\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            compare_perf_intensity_per_year(forecast2 = forecast2, xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', year = year)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "\n",
    "def train_xgb_intensity_all_years_full_train(forecast2 = None, basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'HWRF'):\n",
    "    train = X_train_total\n",
    "    #test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    if sparse:\n",
    "        train = X_train_total_sparse_x\n",
    "        #test = X_test_total_sparse_x\n",
    "    if basin_only:\n",
    "        train = X_train_total[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "        tgt_train = tgt_intensity_train[X_train['cat_basin_'+basin+'_0'] == 1]\n",
    "    xgb_total_1 = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total_1.fit(train, tgt_train)\n",
    "    dict = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[]}\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            index = X_test_baseline.loc[X_test_baseline['YEAR_0'] < year].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "            X_test_to_train = X_test_total[index]\n",
    "            train = np.concatenate((X_train_total, X_test_to_train), axis = 0)\n",
    "            tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[index]), axis = 0)\n",
    "            xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                     subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_total.fit(train, tgt_train)\n",
    "            compare_perf_intensity_per_year(dict = dict, xgb_tot = xgb_total_1, forecast2 = forecast2, xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', year = year)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATS_TRAIN = np.array(X_train['LAT_7']) + np.array(tgt_displacement_train[:,0])*std_dx+mean_dx\n",
    "LONS_TRAIN = np.array(X_train['LON_7']) + np.array(tgt_displacement_train[:,0])*std_dy+mean_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-34.    , -34.9872, -35.9178, ...,  40.1   ,  40.8061,  41.5   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LATS_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train_total\n",
    "train_y = X_train_total\n",
    "test_x = X_test_total\n",
    "test_y = X_test_total\n",
    "tgt_train = tgt_displacement_train\n",
    "\n",
    "xgb_x = XGBRegressor(max_depth=7, n_estimators=90, learning_rate=0.07, subsample=0.7, min_child_weight=5)\n",
    "xgb_x.fit(train_x, LATS_TRAIN_RAD)\n",
    "xgb_y = XGBRegressor(max_depth=7, n_estimators=90, learning_rate=0.07, subsample=0.7, min_child_weight=5)\n",
    "xgb_y.fit(train_y, LONS_TRAIN_RAD)\n",
    "LATS_PRED_RAD = xgb_x.predict(test_x)\n",
    "LONS_PRED_RAD = xgb_y.predict(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATS_TEST_ = X_test['LAT_7'] + np.array(tgt_displacement_test[:, 0])*std_dx+mean_dx\n",
    "LONS_TEST_ = X_test['LON_7'] + np.array(tgt_displacement_test[:, 1])*std_dy+mean_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATS_TRAIN_RAD = np.cos(np.pi / 180 * LATS_TRAIN)\n",
    "LONS_TRAIN_RAD = np.cos(np.pi / 180 * LONS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATS_PRED_ = 180 * np.arccos(LATS_PRED_RAD) / np.pi\n",
    "LONS_PRED_ = 180 * np.arccos(LONS_PRED_RAD) / np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_km_pred = np.zeros(len(LONS_PRED_))\n",
    "for i in range(len(LATS_PRED_)):\n",
    "    d_km_pred[i] = get_distance_km(LONS_PRED_[i], LATS_PRED_[i], LONS_TEST_[i], LATS_TEST_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1468.4680797683357"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_km_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(LONS_PRED_RAD)):\n",
    "    if LONS_PRED_RAD[i] > 1:\n",
    "        LONS_PRED_RAD[i] = 0.999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(LONS_PRED_)):\n",
    "    if LONS_TEST_[i] < 0:\n",
    "        LONS_PRED_[i] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.07,\n",
       "  'min_child_weight': 5,\n",
       "  'n_estimators': 100,\n",
       "  'subsample': 0.7},\n",
       " -0.2854653795560201)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    " 'min_child_weight':[1,5],\n",
    " 'n_estimators':[100, 120, 150],\n",
    " 'subsample':[0.6,0.7,0.8,],\n",
    " 'learning_rate':[0.07, 0.1, 0.15],\n",
    "}\n",
    "grid = GridSearchCV(estimator = XGBRegressor(learning_rate = 0.07, n_estimators=140, max_depth=8,\n",
    "min_child_weight=1, subsample=0.8, seed=1),\n",
    "param_grid = params, n_jobs=4, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "grid.fit(X_train_total, np.array(tgt_intensity_train))\n",
    "\n",
    "grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_intensity_all_years_full_train_4cast(forecast2 = None, forecast3 = None, forecast4 = None, basin_only = False, sparse = False, max_depth = 8, n_estimators = 140, learning_rate = 0.15, subsample = 0.7, min_child_weight=5, basin = 'AN', forecast = 'HWRF'):\n",
    "    train = X_train_total\n",
    "    #test = X_test_total\n",
    "    tgt_train = tgt_intensity_train\n",
    "    xgb_total_1 = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                             subsample=subsample, min_child_weight=min_child_weight)\n",
    "    xgb_total_1.fit(train, tgt_train)\n",
    "    dict = {'year':[], 'num_samples':[], 'MAEs_full':[], 'std_full':[], 'MAES_2012':[], 'std_2012':[], 'MAES_SHIP':[], 'std_SHIP':[], 'MAES_HWRF':[], 'std_HWRF':[], 'MAES_FSSE':[], 'std_FSSE':[], 'MAES_OFCL':[], 'std_OFCL':[]}\n",
    "    for year in range(2012, 2020):\n",
    "        try:\n",
    "            index = X_test_baseline.loc[X_test_baseline['YEAR_0'] < year].index  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "            X_test_to_train = X_test_total[index]\n",
    "            train = np.concatenate((X_train_total, X_test_to_train), axis = 0)\n",
    "            tgt_train = np.concatenate((tgt_intensity_train, tgt_intensity_test[index]), axis = 0)\n",
    "            xgb_total = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
    "                                     subsample=subsample, min_child_weight=min_child_weight)\n",
    "            xgb_total.fit(train, tgt_train)\n",
    "            compare_perf_intensity_per_year_4cast(dict = dict, xgb_tot = xgb_total_1, forecast2 = forecast2, xgb_total = xgb_total, basin=basin, forecast=forecast, mode='vmax', year = year, forecast3= forecast3, forecast4 = forecast4)\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            print(\"\\n No forecasts for year \", year)\n",
    "    return dict\n",
    "\n",
    "def compare_perf_intensity_per_year_4cast(dict, xgb_tot, xgb_total, year, forecast2, forecast3, forecast4, basin = 'AN', forecast = 'SHIP', mode = 'vmax'):\n",
    "    baseline_ = X_test_baseline.loc[X_test_baseline['YEAR_0'] == year].loc[\n",
    "            X_test_baseline[forecast4 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast3 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast2 + '_24_' + mode + '_7'] > -320].loc[\n",
    "            X_test_baseline[forecast + '_24_' + mode + '_7'] > -320].loc[X_test_baseline['cat_basin_' + basin + '_0'] == 1]  # .loc[#X_test_baseline['SHIP_24_'+mode+'_7'] > 0].index\n",
    "    index = baseline_.index\n",
    "    baseline_1 = baseline_[forecast + '_24_'+mode+'_7']\n",
    "    baseline_2 = baseline_[forecast2 + '_24_' + mode + '_7']\n",
    "    baseline_3 = baseline_[forecast3 + '_24_' + mode + '_7']\n",
    "    baseline_4 = baseline_[forecast4 + '_24_' + mode + '_7']\n",
    "    X_test_withBASELINE_total = X_test_total[index]\n",
    "    if mode == 'vmax':\n",
    "        tgt_ = np.array(tgt_intensity_test[index] * std_ + mean_)\n",
    "        print(\"Total number of steps for comparison: \", len(tgt_))\n",
    "        preds_1 = xgb_tot.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        preds = xgb_total.predict(X_test_withBASELINE_total) * std_ + mean_\n",
    "        #print(\"MAE intensity basin \" + basin + \" X stat vs \"+ forecast + \" : \", mean_absolute_error(tgt_intensity_test_withBASELINE * std_ + mean_,\n",
    "                                                     #xgb.predict(X_test_withBASELINE) * std_ + mean_))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Hurricast trained full: \", np.around(mean_absolute_error(tgt_, preds), decimals = 2), \"with std \", np.around(np.std(tgt_ - preds), decimals=2))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Hurricast trained until 2012: \", np.around(mean_absolute_error(tgt_, preds_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - preds_1), decimals=2))\n",
    "        print(\"Year \", year, \" MAE intensity basin \" + basin + \" Official Forecast \"+ forecast + \" : \",\n",
    "              np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2), \"with std \", np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "        if forecast2 != None:\n",
    "            print(\"Year \", year, \" MAE intensity basin \" + basin + \" Official Forecast \" + forecast2 + \" : \",\n",
    "                np.around(mean_absolute_error(tgt_, baseline_2), decimals=2), \"with std \",\n",
    "                np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "            print(\"Year \", year, \" MAE intensity basin \" + basin + \" Official Forecast \" + forecast3 + \" : \",\n",
    "                np.around(mean_absolute_error(tgt_, baseline_3), decimals=2), \"with std \",\n",
    "                np.around(np.std(tgt_ - baseline_3), decimals=2))\n",
    "            print(\"Year \", year, \" MAE intensity basin \" + basin + \" Official Forecast \" + forecast4 + \" : \",\n",
    "                np.around(mean_absolute_error(tgt_, baseline_4), decimals=2), \"with std \",\n",
    "                np.around(np.std(tgt_ - baseline_4), decimals=2))\n",
    "        append_dict_intensity_4cast(dict, tgt_, preds, preds_1, baseline_1, baseline_2, baseline_3, baseline_4, year)\n",
    "        \n",
    "        \n",
    "def append_dict_intensity_4cast(dict, tgt_, preds, preds_1, baseline_1, baseline_2, baseline_3, baseline_4, year):\n",
    "    dict['year'].append(year)\n",
    "    dict['num_samples'].append(len(tgt_))\n",
    "    dict['MAEs_full'].append(np.around(mean_absolute_error(tgt_, preds), decimals = 2))\n",
    "    dict['std_full'].append(np.around(np.std(tgt_ - preds), decimals = 2))\n",
    "    dict['MAES_2012'].append(np.around(mean_absolute_error(tgt_, preds_1), decimals = 2))\n",
    "    dict['std_2012'].append(np.around(np.std(tgt_ - preds_1), decimals=2))\n",
    "    dict['MAES_SHIP'].append(np.around(mean_absolute_error(tgt_, baseline_1), decimals = 2))\n",
    "    dict['std_SHIP'].append(np.around(np.std(tgt_ - baseline_1), decimals = 2))\n",
    "    dict['MAES_HWRF'].append(np.around(mean_absolute_error(tgt_, baseline_2), decimals=2))\n",
    "    dict['std_HWRF'].append(np.around(np.std(tgt_ - baseline_2), decimals=2))\n",
    "    dict['MAES_OFCL'].append(np.around(mean_absolute_error(tgt_, baseline_3), decimals=2))\n",
    "    dict['std_OFCL'].append(np.around(np.std(tgt_ - baseline_3), decimals=2))\n",
    "    dict['MAES_FSSE'].append(np.around(mean_absolute_error(tgt_, baseline_4), decimals=2))\n",
    "    dict['std_FSSE'].append(np.around(np.std(tgt_ - baseline_4), decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AN\n",
    "{'MAES_2012': [9.11, 11.78, 6.65, 8.61, 10.93, 12.33, 9.77, 10.26],\n",
    " 'MAES_FSSE': [10.37, 8.56, 8.16, 8.0, 8.87, 7.72, 8.82, 8.73],\n",
    " 'MAES_HWRF': [9.56, 19.31, 7.02, 9.52, 10.38, 12.32, 10.79, 10.89],\n",
    " 'MAES_OFCL': [9.02, 14.06, 6.7, 7.93, 8.42, 8.59, 8.53, 8.55],\n",
    " 'MAES_SHIP': [8.1, 9.31, 12.13, 9.44, 10.11, 10.05, 8.8, 9.89],\n",
    " 'MAEs_full': [8.84, 12.67, 6.66, 8.67, 11.06, 12.26, 9.78, 10.37],\n",
    " 'num_samples': [41, 8, 72, 122, 231, 228, 236, 208],\n",
    " 'std_2012': [11.11, 10.61, 8.11, 11.71, 14.56, 15.76, 12.47, 13.18],\n",
    " 'std_FSSE': [11.62, 8.28, 8.09, 11.84, 12.22, 10.07, 12.16, 11.42],\n",
    " 'std_HWRF': [10.79, 12.82, 8.36, 12.95, 14.57, 17.27, 14.97, 14.22],\n",
    " 'std_OFCL': [11.36, 5.44, 8.39, 11.75, 11.4, 11.83, 12.1, 11.53],\n",
    " 'std_SHIP': [11.14, 9.22, 13.58, 13.77, 14.34, 12.65, 12.37, 12.68],\n",
    " 'std_full': [11.07, 11.07, 8.17, 11.44, 14.53, 15.52, 12.79, 13.47],\n",
    " 'year': [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]}\n",
    "\n",
    "{'MAES_2012': [9.55, 13.23, 6.83, 8.68, 11.24, 12.18, 9.73, 10.28],\n",
    " 'MAES_FSSE': [10.37, 8.56, 8.16, 8.0, 8.87, 7.72, 8.82, 8.73],\n",
    " 'MAES_HWRF': [9.56, 19.31, 7.02, 9.52, 10.38, 12.32, 10.79, 10.89],\n",
    " 'MAES_OFCL': [9.02, 14.06, 6.7, 7.93, 8.42, 8.59, 8.53, 8.55],\n",
    " 'MAES_SHIP': [8.1, 9.31, 12.13, 9.44, 10.11, 10.05, 8.8, 9.89],\n",
    " 'MAEs_full': [8.91, 10.02, 6.48, 8.52, 11.21, 12.37, 9.83, 10.32],\n",
    " 'num_samples': [41, 8, 72, 122, 231, 228, 236, 208],\n",
    " 'std_2012': [11.84, 13.37, 8.25, 11.73, 14.86, 15.6, 12.56, 13.21],\n",
    " 'std_FSSE': [11.62, 8.28, 8.09, 11.84, 12.22, 10.07, 12.16, 11.42],\n",
    " 'std_HWRF': [10.79, 12.82, 8.36, 12.95, 14.57, 17.27, 14.97, 14.22],\n",
    " 'std_OFCL': [11.36, 5.44, 8.39, 11.75, 11.4, 11.83, 12.1, 11.53],\n",
    " 'std_SHIP': [11.14, 9.22, 13.58, 13.77, 14.34, 12.65, 12.37, 12.68],\n",
    " 'std_full': [11.27, 9.2, 8.01, 11.47, 14.47, 15.82, 12.74, 13.53],\n",
    " 'year': [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]}\n",
    "\n",
    "{'MAES_2012': [8.56, 11.37, 6.81, 8.61, 11.04, 12.08, 9.75, 10.55],\n",
    " 'MAES_FSSE': [10.37, 8.56, 8.16, 8.0, 8.87, 7.72, 8.82, 8.73],\n",
    " 'MAES_HWRF': [9.56, 19.31, 7.02, 9.52, 10.38, 12.32, 10.79, 10.89],\n",
    " 'MAES_OFCL': [9.02, 14.06, 6.7, 7.93, 8.42, 8.59, 8.53, 8.55],\n",
    " 'MAES_SHIP': [8.1, 9.31, 12.13, 9.44, 10.11, 10.05, 8.8, 9.89],\n",
    " 'MAEs_full': [9.27, 12.28, 6.51, 8.46, 11.25, 12.26, 9.72, 10.41],\n",
    " 'num_samples': [41, 8, 72, 122, 231, 228, 236, 208],\n",
    " 'std_2012': [10.97, 11.19, 8.34, 11.75, 14.43, 15.63, 12.67, 13.55],\n",
    " 'std_FSSE': [11.62, 8.28, 8.09, 11.84, 12.22, 10.07, 12.16, 11.42],\n",
    " 'std_HWRF': [10.79, 12.82, 8.36, 12.95, 14.57, 17.27, 14.97, 14.22],\n",
    " 'std_OFCL': [11.36, 5.44, 8.39, 11.75, 11.4, 11.83, 12.1, 11.53],\n",
    " 'std_SHIP': [11.14, 9.22, 13.58, 13.77, 14.34, 12.65, 12.37, 12.68],\n",
    " 'std_full': [11.91, 9.89, 7.98, 11.16, 14.44, 15.64, 12.85, 13.57],\n",
    " 'year': [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP\n",
    "{'MAES_2012': [22.5, 10.2, 12.01, 9.93, 8.98, 12.73, 4.08],\n",
    " 'MAES_FSSE': [10.67, 10.01, 10.66, 8.51, 7.92, 9.82, 24.89],\n",
    " 'MAES_HWRF': [27.0, 12.02, 13.06, 9.87, 9.97, 12.92, 25.58],\n",
    " 'MAES_OFCL': [10.0, 9.73, 10.89, 8.72, 8.14, 10.18, 26.6],\n",
    " 'MAES_SHIP': [11.33, 11.41, 12.28, 10.38, 7.87, 10.92, 21.71],\n",
    " 'MAEs_full': [20.61, 10.28, 11.46, 9.78, 9.12, 12.17, 3.87],\n",
    " 'num_samples': [6, 273, 274, 315, 167, 357, 40],\n",
    " 'std_2012': [8.98, 13.79, 16.12, 13.36, 12.98, 16.46, 4.31],\n",
    " 'std_FSSE': [12.78, 13.07, 14.73, 11.53, 10.67, 12.65, 20.87],\n",
    " 'std_HWRF': [13.22, 15.44, 17.95, 12.84, 13.0, 15.86, 21.77],\n",
    " 'std_OFCL': [13.74, 13.45, 15.91, 11.89, 11.67, 13.44, 23.02],\n",
    " 'std_SHIP': [9.29, 14.57, 17.5, 13.74, 11.61, 14.83, 21.95],\n",
    " 'std_full': [7.0, 13.89, 15.63, 13.48, 12.95, 15.74, 3.92],\n",
    " 'year': [2013, 2014, 2015, 2016, 2017, 2018, 2019]}\n",
    "\n",
    "{'MAES_2012': [22.98, 10.18, 11.94, 9.86, 9.07, 12.7, 4.16],\n",
    " 'MAES_FSSE': [10.67, 10.01, 10.66, 8.51, 7.92, 9.82, 24.89],\n",
    " 'MAES_HWRF': [27.0, 12.02, 13.06, 9.87, 9.97, 12.92, 25.58],\n",
    " 'MAES_OFCL': [10.0, 9.73, 10.89, 8.72, 8.14, 10.18, 26.6],\n",
    " 'MAES_SHIP': [11.33, 11.41, 12.28, 10.38, 7.87, 10.92, 21.71],\n",
    " 'MAEs_full': [20.71, 10.24, 11.46, 9.77, 9.11, 12.19, 4.18],\n",
    " 'num_samples': [6, 273, 274, 315, 167, 357, 40],\n",
    " 'std_2012': [9.24, 13.75, 16.04, 13.31, 13.08, 16.45, 4.4],\n",
    " 'std_FSSE': [12.78, 13.07, 14.73, 11.53, 10.67, 12.65, 20.87],\n",
    " 'std_HWRF': [13.22, 15.44, 17.95, 12.84, 13.0, 15.86, 21.77],\n",
    " 'std_OFCL': [13.74, 13.45, 15.91, 11.89, 11.67, 13.44, 23.02],\n",
    " 'std_SHIP': [9.29, 14.57, 17.5, 13.74, 11.61, 14.83, 21.95],\n",
    " 'std_full': [7.0, 13.87, 15.62, 13.45, 12.97, 15.76, 4.04],\n",
    " 'year': [2013, 2014, 2015, 2016, 2017, 2018, 2019]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20.61, 10.28, 11.46, 9.78, 9.12, 12.17, 3.87]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(([22.5, 10.2, 12.01, 9.93, 8.98, 12.73, 4.08], [22.98, 10.18, 11.94, 9.86, 9.07, 12.7, 4.16], [20.71, 10.24, 11.46, 9.77, 9.11, 12.19, 4.18], [20.61, 10.28, 11.46, 9.78, 9.12, 12.17, 3.87]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 8\n",
    "predict_at = 16\n",
    "\n",
    "tgt_intensity_cat_train = torch.LongTensor(np.load('../data/y_train_intensity_cat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                      allow_pickle=True))\n",
    "tgt_intensity_cat_test = torch.LongTensor(np.load('../data/y_test_intensity_cat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                     allow_pickle=True))\n",
    "\n",
    "tgt_intensity_train = torch.Tensor(np.load('../data/y_train_intensity_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                  allow_pickle=True))\n",
    "tgt_intensity_test = torch.Tensor(np.load('../data/y_test_intensity_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                 allow_pickle=True))\n",
    "\n",
    "tgt_intensity_cat_baseline_train = torch.LongTensor(np.load('../data/y_train_intensity_cat_baseline_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',  allow_pickle = True))\n",
    "tgt_intensity_cat_baseline_test = torch.LongTensor(np.load('../data/y_test_intensity_cat_baseline_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "\n",
    "tgt_displacement_train = torch.Tensor(np.load('../data/y_train_displacement_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                     allow_pickle=True))\n",
    "tgt_displacement_test = torch.Tensor(np.load('../data/y_test_displacement_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                    allow_pickle=True))\n",
    "\n",
    "tgt_displacement_train_unst = torch.Tensor(np.load('../data/y_train_displacement_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                     allow_pickle=True))\n",
    "tgt_displacement_test_unst = torch.Tensor(np.load('../data/y_test_displacement_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "                                    allow_pickle=True))\n",
    "\n",
    "\n",
    "mean_intensity = tgt_intensity_train.mean()\n",
    "std_intensity = tgt_intensity_train.std()\n",
    "tgt_intensity_train = (tgt_intensity_train - mean_intensity)/std_intensity\n",
    "tgt_intensity_test = (tgt_intensity_test - mean_intensity)/std_intensity\n",
    "\n",
    "\n",
    "###INTENSITY\n",
    "mean_dx = tgt_displacement_train[:,0].mean()\n",
    "std_dx = tgt_displacement_train[:,0].std()\n",
    "tgt_displacement_train[:,0] = (tgt_displacement_train[:,0] - mean_dx)/std_dx\n",
    "tgt_displacement_test[:,0] = (tgt_displacement_test[:,0] - mean_dx)/std_dx\n",
    "std_dx = float(std_dx)\n",
    "mean_dx = float(mean_dx)\n",
    "\n",
    "mean_dy = tgt_displacement_train[:,1].mean()\n",
    "std_dy = tgt_displacement_train[:,1].std()\n",
    "tgt_displacement_train[:,1] = (tgt_displacement_train[:,1] - mean_dy)/std_dy\n",
    "tgt_displacement_test[:,1] = (tgt_displacement_test[:,1] - mean_dy)/std_dy\n",
    "std_dy = float(std_dy)\n",
    "mean_dy = float(mean_dy)\n",
    "\n",
    "X_train = np.load('../data/X_train_stat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "            allow_pickle=True)\n",
    "X_test = np.load('../data/X_test_stat_1980_34_20_120_w' + str(window_size) + '_at_' + str(predict_at) + '.npy',\n",
    "            allow_pickle=True)\n",
    "\n",
    "\n",
    "names = ['LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND',\n",
    "         'STORM_SPEED', 'cat_cos_day', 'cat_sign_day', 'COS_STORM_DIR', 'SIN_STORM_DIR',\n",
    "         'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'cat_storm_category', 'cat_basin_AN',\n",
    "         'cat_basin_EP', 'cat_basin_NI', 'cat_basin_SA',\n",
    "         'cat_basin_SI', 'cat_basin_SP', 'cat_basin_WP', 'cat_nature_DS', 'cat_nature_ET',\n",
    "         'cat_nature_MX', 'cat_nature_NR', 'cat_nature_SS', 'cat_nature_TS',\n",
    "         'STORM_DISPLACEMENT_X', 'STORM_DISPLACEMENT_Y']\n",
    "\n",
    "names_all = names * window_size\n",
    "\n",
    "for i in range(len(names_all)):\n",
    "    names_all[i] += '_' + str(i // 30)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_train.columns = names_all\n",
    "X_test.columns = names_all\n",
    "\n",
    "cols = [c for c in X_train.columns if c.lower()[-2:] == '_0' or c.lower()[:3] != 'cat']\n",
    "\n",
    "X_train = X_train[cols]\n",
    "X_test = X_test[cols]\n",
    "\n",
    "\n",
    "X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_48_2012_v2_w' + str(window_size) + '_at_' + str(predict_at) + '.npy', allow_pickle=True))\n",
    "#X_test_baseline = pd.DataFrame(np.load('../data/X_test_stat_1980_34_20_120_forecast_all_48_clean_w8_at_16.npy', allow_pickle=True))\n",
    "\n",
    "names_baselines = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'DIST2LAND', 'STORM_SPEED', 'cos_day', 'sin_day', 'COS_STORM_DIR', 'SIN_STORM_DIR', 'COS_LAT', 'SIN_LAT', 'COS_LON', 'SIN_LON', 'wind_category', 'DSHP_48_lat', 'DSHP_48_lon', 'DSHP_48_vmax', 'DSHP_48_mslp', 'DSHP_48_COS_LAT', 'DSHP_48_SIN_LAT', 'DSHP_48_COS_LON', 'DSHP_48_SIN_LON', 'OFCL_48_lat', 'OFCL_48_lon', 'OFCL_48_vmax', 'OFCL_48_mslp', 'OFCL_48_COS_LAT', 'OFCL_48_SIN_LAT', 'OFCL_48_COS_LON', 'OFCL_48_SIN_LON', 'UKXI_48_lat', 'UKXI_48_lon', 'UKXI_48_vmax', 'UKXI_48_mslp', 'UKXI_48_COS_LAT', 'UKXI_48_SIN_LAT', 'UKXI_48_COS_LON', 'UKXI_48_SIN_LON', 'CMC_48_lat', 'CMC_48_lon', 'CMC_48_vmax', 'CMC_48_mslp', 'CMC_48_COS_LAT', 'CMC_48_SIN_LAT', 'CMC_48_COS_LON', 'CMC_48_SIN_LON', 'SHIP_48_lat', 'SHIP_48_lon', 'SHIP_48_vmax', 'SHIP_48_mslp', 'SHIP_48_COS_LAT', 'SHIP_48_SIN_LAT', 'SHIP_48_COS_LON', 'SHIP_48_SIN_LON', 'FSSE_48_lat', 'FSSE_48_lon', 'FSSE_48_vmax', 'FSSE_48_mslp', 'FSSE_48_COS_LAT', 'FSSE_48_SIN_LAT', 'FSSE_48_COS_LON', 'FSSE_48_SIN_LON', 'CLP5_48_lat', 'CLP5_48_lon', 'CLP5_48_vmax', 'CLP5_48_mslp', 'CLP5_48_COS_LAT', 'CLP5_48_SIN_LAT', 'CLP5_48_COS_LON', 'CLP5_48_SIN_LON', 'AEMN_48_lat', 'AEMN_48_lon', 'AEMN_48_vmax', 'AEMN_48_mslp', 'AEMN_48_COS_LAT', 'AEMN_48_SIN_LAT', 'AEMN_48_COS_LON', 'AEMN_48_SIN_LON', 'LBAR_48_lat', 'LBAR_48_lon', 'LBAR_48_vmax', 'LBAR_48_mslp', 'LBAR_48_COS_LAT', 'LBAR_48_SIN_LAT', 'LBAR_48_COS_LON', 'LBAR_48_SIN_LON', 'GFDL_48_lat', 'GFDL_48_lon', 'GFDL_48_vmax', 'GFDL_48_mslp', 'GFDL_48_COS_LAT', 'GFDL_48_SIN_LAT', 'GFDL_48_COS_LON', 'GFDL_48_SIN_LON', 'HWRF_48_lat', 'HWRF_48_lon', 'HWRF_48_vmax', 'HWRF_48_mslp', 'HWRF_48_COS_LAT', 'HWRF_48_SIN_LAT', 'HWRF_48_COS_LON', 'HWRF_48_SIN_LON', 'NGPS_48_lat', 'NGPS_48_lon', 'NGPS_48_vmax', 'NGPS_48_mslp', 'NGPS_48_COS_LAT', 'NGPS_48_SIN_LAT', 'NGPS_48_COS_LON', 'NGPS_48_SIN_LON', 'DISPLACEMENT_LAT_CLP5_48', 'DISPLACEMENT_LON_CLP5_48', 'DISPLACEMENT_LAT_SHIP_48', 'DISPLACEMENT_LON_SHIP_48', 'DISPLACEMENT_LAT_DSHP_48', 'DISPLACEMENT_LON_DSHP_48', 'DISPLACEMENT_LAT_LBAR_48', 'DISPLACEMENT_LON_LBAR_48', 'DISPLACEMENT_LAT_CMC_48', 'DISPLACEMENT_LON_CMC_48', 'DISPLACEMENT_LAT_NGPS_48', 'DISPLACEMENT_LON_NGPS_48', 'DISPLACEMENT_LAT_GFDL_48', 'DISPLACEMENT_LON_GFDL_48', 'DISPLACEMENT_LAT_HWRF_48', 'DISPLACEMENT_LON_HWRF_48', 'DISPLACEMENT_LAT_UKXI_48', 'DISPLACEMENT_LON_UKXI_48', 'DISPLACEMENT_LAT_FSSE_48', 'DISPLACEMENT_LON_FSSE_48', 'DISPLACEMENT_LAT_AEMN_48', 'DISPLACEMENT_LON_AEMN_48', 'DISPLACEMENT_LAT_OFCL_48', 'DISPLACEMENT_LON_OFCL_48', 'EMXI_48_lat', 'EMXI_48_lon', 'EMXI_48_vmax', 'EMXI_48_mslp', 'EMXI_48_COS_LAT', 'EMXI_48_SIN_LAT', 'EMXI_48_COS_LON', 'EMXI_48_SIN_LON', 'DISPLACEMENT_LAT_EMXI_48', 'DISPLACEMENT_LON_EMXI_48', 'GFSO_48_lat', 'GFSO_48_lon', 'GFSO_48_vmax', 'GFSO_48_mslp', 'GFSO_48_COS_LAT', 'GFSO_48_SIN_LAT', 'GFSO_48_COS_LON', 'GFSO_48_SIN_LON', 'DISPLACEMENT_LAT_GFSO_48', 'DISPLACEMENT_LON_GFSO_48', 'cat_basin_AN', 'cat_basin_EP', 'basin_NI', 'basin_SI', 'basin_SP', 'basin_WP', 'DISPLACEMENT_LAT', 'DISPLACEMENT_LON']\n",
    "\n",
    "names_all_baselines = names_baselines * 8#args.window_size\n",
    "\n",
    "for i in range(len(names_all_baselines)):\n",
    "    names_all_baselines[i] += '_' + str(i // 167)\n",
    "\n",
    "X_test_baseline.columns = names_all_baselines\n",
    "\n",
    "\n",
    "#X_train_embed = np.load('../data/embeddings/X_train_embed_1980_34_20_120_intensity_48.npy', allow_pickle = True)\n",
    "#X_test_embed = np.load('../data/embeddings/X_test_embed_1980_34_20_120_intensity_48.npy', allow_pickle = True)\n",
    "\n",
    "X_train_embed = np.load('../data/embeddings/X_train_embed_1980_34_20_120_track_48.npy', allow_pickle = True)\n",
    "X_test_embed = np.load('../data/embeddings/X_test_embed_1980_34_20_120_track_48.npy', allow_pickle = True)\n",
    "\n",
    "X_train_total = np.concatenate((X_train, X_train_embed), axis = 1)\n",
    "X_test_total = np.concatenate((X_test, X_test_embed), axis = 1)\n",
    "\n",
    "std_ = float(std_intensity)\n",
    "mean_ = float(mean_intensity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
