{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import os \n",
    "import sys\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2016_2018\u001b[m\u001b[m         \u001b[34mtensor_completion\u001b[m\u001b[m y.npy\r\n",
      "\u001b[34mdata_era\u001b[m\u001b[m          vision_data.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Load the data using the loader. Depending on the mode chosen, the iterator will output different dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset and corresponding sizes (null elements included):\n",
      "X_vision torch.Size([3735, 8, 9, 25, 25])\n",
      "X_stat torch.Size([3735, 8, 10])\n",
      "target_displacement torch.Size([3735, 8, 2])\n",
      "target_intensity torch.Size([3735])\n",
      "target_intensity_cat torch.Size([3735])\n",
      "target_intensity_cat_baseline torch.Size([3735])\n",
      "Keeping 3143 samples out of the initial 3735.\n",
      "Reshaping the displacement target...\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "import imp\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    data_dir: str\n",
    "    y_name: str\n",
    "    vision_name: str\n",
    "    predict_at: int\n",
    "    window_size: int\n",
    "    train_test_split: float\n",
    "    mode: str\n",
    "    batch_size: int\n",
    "        \n",
    "args = Args(data_dir=\"data/\", \n",
    "            y_name=\"y.npy\",\n",
    "           vision_name=\"vision_data.npy\", \n",
    "           predict_at=8,\n",
    "           window_size=8, \n",
    "           train_test_split=0.8, \n",
    "           batch_size=10, \n",
    "           mode='intensity')\n",
    "\n",
    "from src.prepro import create_loaders\n",
    "train_loader, test_loader = create_loaders(**vars(args), debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does create_loaders work ?**\n",
    "```python\n",
    "\n",
    "vision_data = np.load(osp.join(data_dir, vision_name),\n",
    "                          allow_pickle=True)\n",
    "y = np.load(osp.join(data_dir, y_name),\n",
    "                allow_pickle=True)\n",
    "\n",
    "#Create named tensors\n",
    "train_tensors, test_tensors = Prepro.process(\n",
    "        y=y, \n",
    "        vision_data=vision_data,\n",
    "        train_split=train_test_split,\n",
    "        predict_at=predict_at,\n",
    "        window_size=window_size)\n",
    "\n",
    "#Filter the relevant keys\n",
    "train_tensors, test_tensors = filter_keys(\n",
    "     train_tensors, test_tensors, mode=mode)\n",
    "\n",
    "#Unroll in tensordataset\n",
    "train_ds = TensorDataset(*train_tensors.values())\n",
    "test_ds = TensorDataset(*test_tensors.values())\n",
    "    \n",
    "#Create collate_fn \n",
    "collate_fn = create_collate_fn()\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True, \n",
    "                            drop_last=True,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_ds, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False, \n",
    "                            collate_fn=collate_fn)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_model x_viz torch.Size([10, 8, 9, 25, 25])\n",
      "in_model x_stat torch.Size([10, 8, 10])\n",
      "in_loss trg_y torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "in_model, in_loss = next(iter(train_loader))\n",
    "for k, v in in_model.items(): print('in_model', k, v.size())\n",
    "for k, v in in_loss.items(): print('in_loss', k, v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(in_model['x_viz'][0,0,1].detach().numpy());plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Models\n",
    "Experimental version in experimental_models (TO COME, see below).\n",
    "___\n",
    "The different configurations are stored in scripts/config.py.\n",
    "We create a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#import imp\\n#import src\\n#imp.reload(src.models)\\n#imp.reload(src.models.factory)\\n#imp.reload(src.models.hurricast_models)\\n#import src.models\\n#del args.device\\n#del args.writer\\nfrom dataclasses import dataclass, field, asdict\\nimport imp\\n\\n@dataclass\\nclass Args:\\n    data_dir: str\\n    y_name: str\\n    vision_name: str\\n    predict_at: int\\n    window_size: int\\n    train_test_split: float\\n    mode: str\\n    batch_size: int\\n        \\nargs = Args(data_dir=\"data/\", \\n            y_name=\"y.npy\",\\n           vision_name=\"vision_data.npy\", \\n           predict_at=8,\\n           window_size=8, \\n           train_test_split=0.8, \\n           batch_size=10, \\n           mode=\\'intensity\\')\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#import imp\n",
    "#import src\n",
    "#imp.reload(src.models)\n",
    "#imp.reload(src.models.factory)\n",
    "#imp.reload(src.models.hurricast_models)\n",
    "#import src.models\n",
    "#del args.device\n",
    "#del args.writer\n",
    "from dataclasses import dataclass, field, asdict\n",
    "import imp\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    data_dir: str\n",
    "    y_name: str\n",
    "    vision_name: str\n",
    "    predict_at: int\n",
    "    window_size: int\n",
    "    train_test_split: float\n",
    "    mode: str\n",
    "    batch_size: int\n",
    "        \n",
    "args = Args(data_dir=\"data/\", \n",
    "            y_name=\"y.npy\",\n",
    "           vision_name=\"vision_data.npy\", \n",
    "           predict_at=8,\n",
    "           window_size=8, \n",
    "           train_test_split=0.8, \n",
    "           batch_size=10, \n",
    "           mode='intensity')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prepare the training using  cpu\n"
     ]
    }
   ],
   "source": [
    "#if args.encdec: decoder_config = config.encdec_config\n",
    "#elif args.transformer: decoder_config=config.transformer_config\n",
    "#else: decoder_config = config.lineartransform_config\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from scripts import config\n",
    "from src.models import factory, hurricast_models\n",
    "from src import setup\n",
    "#Just a hack for the notebook\n",
    "args.encdec = True\n",
    "args.transformer = False\n",
    "args.output_dir = 'results/companion_notebook'\n",
    "writer = setup.create_board(args)\n",
    "device = setup.create_device(gpu_nb=-1)\n",
    "args.device = None\n",
    "args.writer = writer\n",
    "# End of the hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENCDEC(\n",
       "  (encoder): CNNEncoder(\n",
       "    (activation): ReLU()\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU()\n",
       "      (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (11): Flatten()\n",
       "      (12): Linear(in_features=4096, out_features=256, bias=True)\n",
       "      (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU()\n",
       "      (15): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder_cells): Sequential(\n",
       "    (0): GRUCell(138, 128)\n",
       "    (1): GRUCell(128, 128)\n",
       "  )\n",
       "  (last_linear): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_config = config.encoder_config\n",
    "decoder_config = config.encdec_config\n",
    "\n",
    "model = factory.get_model(\n",
    "        mode=args.mode, \n",
    "        encoder_config=encoder_config,\n",
    "        decoder_config=decoder_config, \n",
    "        args=args)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the refistered models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CNNEncoder': <class 'src.models.hurricast_models.CNNEncoder'>, 'ENCDEC': <class 'src.models.hurricast_models.ENCDEC'>, 'TRANSFORMER': <class 'src.models.hurricast_models.TRANSFORMER'>, 'LINEARTransform': <class 'src.models.hurricast_models.LINEARTransform'>}\n"
     ]
    }
   ],
   "source": [
    "print(factory.MODEL_REGISTRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model creation is wrapped using the models/factory.py file and the function:**\n",
    "```py\n",
    "def get_model(mode, encoder_config, decoder_config, args):\n",
    "    #Needs to upload window size and _OUT_DECODER\n",
    "    #Get some\n",
    "    assert (int(args.encdec) + int(args.transformer) < 2), \"\\\n",
    "    Only one of encdec or transformer can be specified\"\n",
    "\n",
    "    _encoder = MODEL_REGISTRY['CNNEncoder']\n",
    "    if args.encdec:\n",
    "        _model = MODEL_REGISTRY['ENCDEC']\n",
    "    elif args.transformer:\n",
    "        _model = MODEL_REGISTRY['TRANSFORMER']\n",
    "    else:\n",
    "        model = MODEL_REGISTRY['LINEARTransform']\n",
    "\n",
    "    N_OUT_DECODER = 7 if mode == 'intensity_cat' else (\n",
    "        2 - (mode == 'intensity'))  # 7 classes of storms if categorical\n",
    "\n",
    "    #Encoder\n",
    "    encoder_config = encoder_config if isinstance(encoder_config, dict)\\\n",
    "        else vars(encoder_config)\n",
    "\n",
    "    encoder = _encoder(**encoder_config)\n",
    "\n",
    "    #Decoder: Update the config\n",
    "    decoder_config = decoder_config if isinstance(decoder_config, dict)\\\n",
    "        else vars(encoder_config)\n",
    "\n",
    "    if not args.encdec and not args.transformer:\n",
    "        decoder_config['target_intensity'] = args.target_intensity,\n",
    "        decoder_config['target_intensity_cat'] = args.target_intensity_cat\n",
    "\n",
    "    else:\n",
    "        decoder_config['encoder'] = encoder\n",
    "        decoder_config['window_size'] = args.window_size\n",
    "        decoder_config['n_out_decoder'] = N_OUT_DECODER\n",
    "\n",
    "    model = _model(**decoder_config)\n",
    "\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    if args.writer is not None:\n",
    "        configs = [encoder_config, decoder_config]\n",
    "        config_ = \"\"\n",
    "        for config__ in configs:\n",
    "            config_ += \"{}\\n\".format(config__)\n",
    "        args.writer.add_text('Configs', config_)\n",
    "    return model\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3  - Prepare the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Inner Training loop:   0%|          | 0/251 [00:00<?, ?it/s]\u001b[A/Users/theoguenais/anaconda/envs/graphcnn/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\n",
      "Inner Training loop:   0%|          | 1/251 [00:00<03:24,  1.22it/s]\u001b[A\n",
      "Inner Training loop:   1%|          | 2/251 [00:01<03:25,  1.21it/s]\u001b[A\n",
      "Inner Training loop:   1%|          | 3/251 [00:02<03:34,  1.16it/s]\u001b[A\n",
      "Inner Training loop:   2%|▏         | 4/251 [00:03<03:36,  1.14it/s]\u001b[A\n",
      "Inner Training loop:   2%|▏         | 5/251 [00:04<03:43,  1.10it/s]\u001b[A\n",
      "Inner Training loop:   2%|▏         | 6/251 [00:05<03:44,  1.09it/s]\u001b[A\n",
      "Inner Training loop:   3%|▎         | 7/251 [00:06<03:47,  1.07it/s]\u001b[A\n",
      "Inner Training loop:   3%|▎         | 8/251 [00:07<03:50,  1.05it/s]\u001b[A\n",
      "Inner Training loop:   4%|▎         | 9/251 [00:08<04:01,  1.00it/s]\u001b[A\n",
      "Inner Training loop:   4%|▍         | 10/251 [00:09<03:54,  1.03it/s]\u001b[A\n",
      "Inner Training loop:   4%|▍         | 11/251 [00:10<03:51,  1.04it/s]\u001b[A\n",
      "Inner Training loop:   5%|▍         | 12/251 [00:11<03:46,  1.06it/s]\u001b[A\n",
      "Inner Training loop:   5%|▌         | 13/251 [00:12<03:37,  1.09it/s]\u001b[A\n",
      "Inner Training loop:   6%|▌         | 14/251 [00:12<03:23,  1.16it/s]\u001b[A\n",
      "Inner Training loop:   6%|▌         | 15/251 [00:13<03:16,  1.20it/s]\u001b[A\n",
      "Inner Training loop:   6%|▋         | 16/251 [00:14<03:06,  1.26it/s]\u001b[A\n",
      "Inner Training loop:   7%|▋         | 17/251 [00:15<02:58,  1.31it/s]\u001b[A\n",
      "Inner Training loop:   7%|▋         | 18/251 [00:15<02:53,  1.34it/s]\u001b[A\n",
      "Inner Training loop:   8%|▊         | 19/251 [00:16<02:49,  1.37it/s]\u001b[A\n",
      "Inner Training loop:   8%|▊         | 20/251 [00:17<03:10,  1.21it/s]\u001b[A\n",
      "Inner Training loop:   8%|▊         | 21/251 [00:18<03:19,  1.15it/s]\u001b[A\n",
      "Inner Training loop:   9%|▉         | 22/251 [00:19<03:16,  1.17it/s]\u001b[A\n",
      "Inner Training loop:   9%|▉         | 23/251 [00:20<03:22,  1.13it/s]\u001b[A\n",
      "Inner Training loop:  10%|▉         | 24/251 [00:21<03:20,  1.13it/s]\u001b[A\n",
      "Inner Training loop:  10%|▉         | 25/251 [00:21<03:10,  1.18it/s]\u001b[A\n",
      "Inner Training loop:  10%|█         | 26/251 [00:22<03:06,  1.21it/s]\u001b[A\n",
      "Inner Training loop:  11%|█         | 27/251 [00:23<03:02,  1.23it/s]\u001b[A\n",
      "Inner Training loop:  11%|█         | 28/251 [00:24<02:53,  1.29it/s]\u001b[A\n",
      "Inner Training loop:  12%|█▏        | 29/251 [00:24<02:47,  1.33it/s]\u001b[A\n",
      "Inner Training loop:  12%|█▏        | 30/251 [00:25<02:41,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  12%|█▏        | 31/251 [00:26<02:38,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  13%|█▎        | 32/251 [00:26<02:36,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  13%|█▎        | 33/251 [00:27<02:38,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  14%|█▎        | 34/251 [00:28<02:39,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  14%|█▍        | 35/251 [00:29<02:37,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  14%|█▍        | 36/251 [00:29<02:34,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  15%|█▍        | 37/251 [00:30<02:32,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  15%|█▌        | 38/251 [00:31<02:30,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  16%|█▌        | 39/251 [00:31<02:29,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  16%|█▌        | 40/251 [00:32<02:30,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  16%|█▋        | 41/251 [00:33<02:33,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  17%|█▋        | 42/251 [00:34<02:30,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  17%|█▋        | 43/251 [00:34<02:29,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  18%|█▊        | 44/251 [00:35<02:27,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  18%|█▊        | 45/251 [00:36<02:26,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  18%|█▊        | 46/251 [00:36<02:26,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  19%|█▊        | 47/251 [00:37<02:26,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  19%|█▉        | 48/251 [00:38<02:31,  1.34it/s]\u001b[A\n",
      "Inner Training loop:  20%|█▉        | 49/251 [00:39<02:27,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  20%|█▉        | 50/251 [00:39<02:26,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  20%|██        | 51/251 [00:40<02:23,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  21%|██        | 52/251 [00:41<02:21,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  21%|██        | 53/251 [00:42<02:25,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  22%|██▏       | 54/251 [00:43<02:40,  1.22it/s]\u001b[A\n",
      "Inner Training loop:  22%|██▏       | 55/251 [00:43<02:43,  1.20it/s]\u001b[A\n",
      "Inner Training loop:  22%|██▏       | 56/251 [00:44<02:36,  1.25it/s]\u001b[A\n",
      "Inner Training loop:  23%|██▎       | 57/251 [00:45<02:29,  1.30it/s]\u001b[A\n",
      "Inner Training loop:  23%|██▎       | 58/251 [00:46<02:24,  1.34it/s]\u001b[A\n",
      "Inner Training loop:  24%|██▎       | 59/251 [00:46<02:21,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  24%|██▍       | 60/251 [00:47<02:21,  1.35it/s]\u001b[A\n",
      "Inner Training loop:  24%|██▍       | 61/251 [00:48<02:20,  1.35it/s]\u001b[A\n",
      "Inner Training loop:  25%|██▍       | 62/251 [00:48<02:18,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  25%|██▌       | 63/251 [00:49<02:16,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  25%|██▌       | 64/251 [00:50<02:14,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  26%|██▌       | 65/251 [00:51<02:13,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  26%|██▋       | 66/251 [00:51<02:12,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  27%|██▋       | 67/251 [00:52<02:14,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  27%|██▋       | 68/251 [00:53<02:16,  1.34it/s]\u001b[A\n",
      "Inner Training loop:  27%|██▋       | 69/251 [00:54<02:13,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  28%|██▊       | 70/251 [00:54<02:11,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  28%|██▊       | 71/251 [00:55<02:09,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  29%|██▊       | 72/251 [00:56<02:08,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  29%|██▉       | 73/251 [00:56<02:06,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  29%|██▉       | 74/251 [00:57<02:08,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  30%|██▉       | 75/251 [00:58<02:09,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  30%|███       | 76/251 [00:59<02:07,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  31%|███       | 77/251 [00:59<02:05,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  31%|███       | 78/251 [01:00<02:02,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  31%|███▏      | 79/251 [01:01<02:01,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  32%|███▏      | 80/251 [01:01<02:01,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  32%|███▏      | 81/251 [01:02<02:02,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  33%|███▎      | 82/251 [01:03<02:04,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  33%|███▎      | 83/251 [01:04<02:02,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  33%|███▎      | 84/251 [01:04<02:00,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  34%|███▍      | 85/251 [01:05<01:59,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  34%|███▍      | 86/251 [01:06<01:58,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  35%|███▍      | 87/251 [01:07<01:59,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  35%|███▌      | 88/251 [01:07<01:57,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  35%|███▌      | 89/251 [01:08<01:59,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  36%|███▌      | 90/251 [01:09<01:57,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  36%|███▋      | 91/251 [01:09<01:55,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  37%|███▋      | 92/251 [01:10<01:53,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  37%|███▋      | 93/251 [01:11<01:52,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  37%|███▋      | 94/251 [01:12<01:54,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  38%|███▊      | 95/251 [01:12<01:51,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  38%|███▊      | 96/251 [01:13<01:52,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  39%|███▊      | 97/251 [01:14<01:50,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  39%|███▉      | 98/251 [01:14<01:50,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  39%|███▉      | 99/251 [01:15<01:49,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  40%|███▉      | 100/251 [01:16<01:48,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  40%|████      | 101/251 [01:17<01:49,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  41%|████      | 102/251 [01:17<01:47,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  41%|████      | 103/251 [01:18<01:48,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  41%|████▏     | 104/251 [01:19<01:46,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  42%|████▏     | 105/251 [01:20<01:44,  1.39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inner Training loop:  42%|████▏     | 106/251 [01:20<01:43,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  43%|████▎     | 107/251 [01:21<01:43,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  43%|████▎     | 108/251 [01:22<01:44,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  43%|████▎     | 109/251 [01:22<01:42,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  44%|████▍     | 110/251 [01:23<01:42,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  44%|████▍     | 111/251 [01:24<01:40,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  45%|████▍     | 112/251 [01:25<01:38,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  45%|████▌     | 113/251 [01:25<01:37,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  45%|████▌     | 114/251 [01:26<01:36,  1.43it/s]\u001b[A\n",
      "Inner Training loop:  46%|████▌     | 115/251 [01:27<01:37,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  46%|████▌     | 116/251 [01:27<01:36,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  47%|████▋     | 117/251 [01:28<01:36,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  47%|████▋     | 118/251 [01:29<01:34,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  47%|████▋     | 119/251 [01:30<01:33,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  48%|████▊     | 120/251 [01:30<01:32,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  48%|████▊     | 121/251 [01:31<01:30,  1.43it/s]\u001b[A\n",
      "Inner Training loop:  49%|████▊     | 122/251 [01:32<01:32,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  49%|████▉     | 123/251 [01:32<01:31,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  49%|████▉     | 124/251 [01:33<01:31,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  50%|████▉     | 125/251 [01:34<01:29,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  50%|█████     | 126/251 [01:35<01:28,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  51%|█████     | 127/251 [01:35<01:27,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  51%|█████     | 128/251 [01:36<01:26,  1.43it/s]\u001b[A\n",
      "Inner Training loop:  51%|█████▏    | 129/251 [01:37<01:28,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  52%|█████▏    | 130/251 [01:37<01:26,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  52%|█████▏    | 131/251 [01:38<01:27,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  53%|█████▎    | 132/251 [01:39<01:26,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  53%|█████▎    | 133/251 [01:40<01:25,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  53%|█████▎    | 134/251 [01:40<01:30,  1.29it/s]\u001b[A\n",
      "Inner Training loop:  54%|█████▍    | 135/251 [01:41<01:28,  1.31it/s]\u001b[A\n",
      "Inner Training loop:  54%|█████▍    | 136/251 [01:42<01:27,  1.31it/s]\u001b[A\n",
      "Inner Training loop:  55%|█████▍    | 137/251 [01:43<01:25,  1.34it/s]\u001b[A\n",
      "Inner Training loop:  55%|█████▍    | 138/251 [01:43<01:24,  1.33it/s]\u001b[A\n",
      "Inner Training loop:  55%|█████▌    | 139/251 [01:44<01:22,  1.35it/s]\u001b[A\n",
      "Inner Training loop:  56%|█████▌    | 140/251 [01:45<01:21,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  56%|█████▌    | 141/251 [01:46<01:19,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  57%|█████▋    | 142/251 [01:46<01:20,  1.35it/s]\u001b[A\n",
      "Inner Training loop:  57%|█████▋    | 143/251 [01:47<01:20,  1.34it/s]\u001b[A\n",
      "Inner Training loop:  57%|█████▋    | 144/251 [01:48<01:20,  1.33it/s]\u001b[A\n",
      "Inner Training loop:  58%|█████▊    | 145/251 [01:49<01:18,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  58%|█████▊    | 146/251 [01:49<01:16,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  59%|█████▊    | 147/251 [01:50<01:14,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  59%|█████▉    | 148/251 [01:51<01:14,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  59%|█████▉    | 149/251 [01:51<01:12,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  60%|█████▉    | 150/251 [01:52<01:12,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  60%|██████    | 151/251 [01:53<01:13,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  61%|██████    | 152/251 [01:54<01:11,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  61%|██████    | 153/251 [01:54<01:10,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  61%|██████▏   | 154/251 [01:55<01:08,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  62%|██████▏   | 155/251 [01:56<01:11,  1.35it/s]\u001b[A\n",
      "Inner Training loop:  62%|██████▏   | 156/251 [01:57<01:14,  1.28it/s]\u001b[A\n",
      "Inner Training loop:  63%|██████▎   | 157/251 [01:58<01:14,  1.26it/s]\u001b[A\n",
      "Inner Training loop:  63%|██████▎   | 158/251 [01:58<01:13,  1.26it/s]\u001b[A\n",
      "Inner Training loop:  63%|██████▎   | 159/251 [01:59<01:10,  1.31it/s]\u001b[A\n",
      "Inner Training loop:  64%|██████▎   | 160/251 [02:00<01:07,  1.35it/s]\u001b[A\n",
      "Inner Training loop:  64%|██████▍   | 161/251 [02:00<01:05,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  65%|██████▍   | 162/251 [02:01<01:03,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  65%|██████▍   | 163/251 [02:02<01:04,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  65%|██████▌   | 164/251 [02:03<01:02,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  66%|██████▌   | 165/251 [02:03<01:02,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  66%|██████▌   | 166/251 [02:04<01:01,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  67%|██████▋   | 167/251 [02:05<00:59,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  67%|██████▋   | 168/251 [02:05<00:58,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  67%|██████▋   | 169/251 [02:06<00:57,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  68%|██████▊   | 170/251 [02:07<00:58,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  68%|██████▊   | 171/251 [02:08<00:57,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  69%|██████▊   | 172/251 [02:08<00:57,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  69%|██████▉   | 173/251 [02:09<00:55,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  69%|██████▉   | 174/251 [02:10<00:54,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  70%|██████▉   | 175/251 [02:10<00:53,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  70%|███████   | 176/251 [02:11<00:52,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  71%|███████   | 177/251 [02:12<00:53,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  71%|███████   | 178/251 [02:13<00:51,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  71%|███████▏  | 179/251 [02:13<00:52,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  72%|███████▏  | 180/251 [02:14<00:50,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  72%|███████▏  | 181/251 [02:15<00:49,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  73%|███████▎  | 182/251 [02:15<00:48,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  73%|███████▎  | 183/251 [02:16<00:47,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  73%|███████▎  | 184/251 [02:17<00:50,  1.32it/s]\u001b[A\n",
      "Inner Training loop:  74%|███████▎  | 185/251 [02:18<00:55,  1.18it/s]\u001b[A\n",
      "Inner Training loop:  74%|███████▍  | 186/251 [02:19<00:56,  1.16it/s]\u001b[A\n",
      "Inner Training loop:  75%|███████▍  | 187/251 [02:20<00:56,  1.13it/s]\u001b[A\n",
      "Inner Training loop:  75%|███████▍  | 188/251 [02:21<00:56,  1.11it/s]\u001b[A\n",
      "Inner Training loop:  75%|███████▌  | 189/251 [02:22<00:54,  1.14it/s]\u001b[A\n",
      "Inner Training loop:  76%|███████▌  | 190/251 [02:22<00:51,  1.19it/s]\u001b[A\n",
      "Inner Training loop:  76%|███████▌  | 191/251 [02:23<00:49,  1.22it/s]\u001b[A\n",
      "Inner Training loop:  76%|███████▋  | 192/251 [02:24<00:46,  1.28it/s]\u001b[A\n",
      "Inner Training loop:  77%|███████▋  | 193/251 [02:24<00:43,  1.33it/s]\u001b[A\n",
      "Inner Training loop:  77%|███████▋  | 194/251 [02:25<00:41,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  78%|███████▊  | 195/251 [02:26<00:40,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  78%|███████▊  | 196/251 [02:27<00:40,  1.36it/s]\u001b[A\n",
      "Inner Training loop:  78%|███████▊  | 197/251 [02:27<00:38,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  79%|███████▉  | 198/251 [02:28<00:38,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  79%|███████▉  | 199/251 [02:29<00:37,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  80%|███████▉  | 200/251 [02:29<00:36,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  80%|████████  | 201/251 [02:30<00:35,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  80%|████████  | 202/251 [02:31<00:34,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  81%|████████  | 203/251 [02:32<00:34,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  81%|████████▏ | 204/251 [02:32<00:33,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  82%|████████▏ | 205/251 [02:33<00:33,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  82%|████████▏ | 206/251 [02:34<00:32,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  82%|████████▏ | 207/251 [02:34<00:31,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  83%|████████▎ | 208/251 [02:35<00:30,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  83%|████████▎ | 209/251 [02:36<00:29,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  84%|████████▎ | 210/251 [02:37<00:29,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  84%|████████▍ | 211/251 [02:37<00:28,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  84%|████████▍ | 212/251 [02:38<00:28,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  85%|████████▍ | 213/251 [02:39<00:27,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  85%|████████▌ | 214/251 [02:39<00:26,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  86%|████████▌ | 215/251 [02:40<00:25,  1.41it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inner Training loop:  86%|████████▌ | 216/251 [02:41<00:24,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  86%|████████▋ | 217/251 [02:42<00:24,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  87%|████████▋ | 218/251 [02:42<00:23,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  87%|████████▋ | 219/251 [02:43<00:23,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  88%|████████▊ | 220/251 [02:44<00:22,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  88%|████████▊ | 221/251 [02:44<00:21,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  88%|████████▊ | 222/251 [02:45<00:20,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  89%|████████▉ | 223/251 [02:46<00:19,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  89%|████████▉ | 224/251 [02:47<00:19,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  90%|████████▉ | 225/251 [02:47<00:18,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  90%|█████████ | 226/251 [02:48<00:18,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  90%|█████████ | 227/251 [02:49<00:17,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  91%|█████████ | 228/251 [02:50<00:16,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  91%|█████████ | 229/251 [02:50<00:15,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  92%|█████████▏| 230/251 [02:51<00:15,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  92%|█████████▏| 231/251 [02:52<00:14,  1.37it/s]\u001b[A\n",
      "Inner Training loop:  92%|█████████▏| 232/251 [02:52<00:13,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  93%|█████████▎| 233/251 [02:53<00:13,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  93%|█████████▎| 234/251 [02:54<00:12,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  94%|█████████▎| 235/251 [02:55<00:11,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  94%|█████████▍| 236/251 [02:55<00:10,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  94%|█████████▍| 237/251 [02:56<00:09,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  95%|█████████▍| 238/251 [02:57<00:09,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  95%|█████████▌| 239/251 [02:57<00:08,  1.41it/s]\u001b[A\n",
      "Inner Training loop:  96%|█████████▌| 240/251 [02:58<00:07,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  96%|█████████▌| 241/251 [02:59<00:07,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  96%|█████████▋| 242/251 [03:00<00:06,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  97%|█████████▋| 243/251 [03:00<00:05,  1.42it/s]\u001b[A\n",
      "Inner Training loop:  97%|█████████▋| 244/251 [03:01<00:04,  1.43it/s]\u001b[A\n",
      "Inner Training loop:  98%|█████████▊| 245/251 [03:02<00:04,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  98%|█████████▊| 246/251 [03:02<00:03,  1.40it/s]\u001b[A\n",
      "Inner Training loop:  98%|█████████▊| 247/251 [03:03<00:02,  1.38it/s]\u001b[A\n",
      "Inner Training loop:  99%|█████████▉| 248/251 [03:04<00:02,  1.39it/s]\u001b[A\n",
      "Inner Training loop:  99%|█████████▉| 249/251 [03:05<00:01,  1.41it/s]\u001b[A\n",
      "Inner Training loop: 100%|█████████▉| 250/251 [03:05<00:00,  1.41it/s]\u001b[A\n",
      "Inner Training loop: 100%|██████████| 251/251 [03:06<00:00,  1.35it/s]\u001b[A\n",
      "/Users/theoguenais/anaconda/envs/graphcnn/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([629])) that is different to the input size (torch.Size([629, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 0 | Loss 0.9798471331596375: 100%|██████████| 1/1 [03:14<00:00, 194.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not write confusion_matrix due to name 'workspace' is not defined\n",
      "Could not write accuracy due to name 'workspace' is not defined\n",
      "Could not write precision due to name 'workspace' is not defined\n",
      "Could not write recall due to name 'workspace' is not defined\n",
      "Could not write f1 due to name 'workspace' is not defined\n",
      "Could not write f1_micro due to name 'workspace' is not defined\n",
      "Could not write f1_macro due to name 'workspace' is not defined\n",
      "Could not write classification_report due to name 'workspace' is not defined\n",
      "Epoch: 01 | Time: 3m 14s\n",
      "\tTrain Loss: 1.0093 | Train PPL:   2.744\n",
      "\t Val. Loss: 0.980 |  Val. PPL:   2.664\n",
      "\t Final test ACC 0.980\n",
      "Problem 'Args' object has no attribute 'lr'\n",
      "Problem 'Args' object has no attribute 'l2_reg'\n",
      "Problem 'Args' object has no attribute 'lr'\n",
      "Problem 'Args' object has no attribute 'model'\n",
      "Problem 'Args' object has no attribute 'num_epochs'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theoguenais/anaconda/envs/graphcnn/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([629])) that is different to the input size (torch.Size([629, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "#Hack for the notebook\n",
    "from src import metrics, run\n",
    "args.n_epochs = 1\n",
    "args.get_training_stats = True\n",
    "task = 'regression'\n",
    "train_loss_fn, \\\n",
    "eval_loss_fn, metrics_fn = metrics.create_metrics_fn(task)\n",
    "\n",
    "best_model, \\\n",
    "        optimizer, \\\n",
    "        training_stats = run.train(\n",
    "            model, optimizer,\n",
    "            num_epochs=args.n_epochs,\n",
    "            train_loss_fn=train_loss_fn,\n",
    "            test_loss_fn=eval_loss_fn,\n",
    "            metrics_fn=metrics_fn,\n",
    "            train_iterator=train_loader,\n",
    "            val_iterator=test_loader,\n",
    "            test_iterator=test_loader,\n",
    "            mode=args.mode,\n",
    "            task=task,\n",
    "            get_training_stats=args.get_training_stats,\n",
    "            clip=None,\n",
    "            scheduler=None,\n",
    "            l2_reg=0.,\n",
    "            save=False,\n",
    "            args=args,\n",
    "            output_dir=args.output_dir,\n",
    "            writer=args.writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the training work exactly?\n",
    "1. **We Backprop and train for one epoch**\n",
    "```py\n",
    "train_losses, \\\n",
    "preds, true_preds = train_epoch(\n",
    "                        model=model,\n",
    "                        train_iterator=train_iterator,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=train_loss_fn,\n",
    "                        global_step=epoch,\n",
    "                        task=get_training_stats,\n",
    "                        return_pt=True,\n",
    "                        l2_reg=l2_reg,\n",
    "                        clip=clip,\n",
    "                        scheduler=scheduler)\n",
    "```\n",
    "2. **Obtain the training metrics.**\n",
    "```py\n",
    "train_metrics = metrics_fn(preds, true_preds)\n",
    "```   \n",
    "3. **Eval (metrics obtained in the function directly).**\n",
    "```py\n",
    "preds, true_preds, \\\n",
    "valid_loss, eval_metrics = evaluate(\n",
    "                                    model=model, \n",
    "                                    iterator=val_iterator,\n",
    "                                    loss_fn=test_loss_fn,\n",
    "                                    metrics_func=metrics_fn,\n",
    "                                    task=task)\n",
    "```\n",
    "\n",
    "**We repeat the above for each epoch.**\n",
    "\n",
    "The final output of the training is: \n",
    "```py\n",
    "best_model, optimizer, training_stats : dict = {\n",
    "                                'train_metrics': ...,\n",
    "                                'eval_metrics': ...,\n",
    "                                'test_metrics': ...,\n",
    "                                'test_preds': ...,\n",
    "                                'test_labels': ...\n",
    "                                }\n",
    "```                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
